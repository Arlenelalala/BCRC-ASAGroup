# 学习内容

* 文本摘要

  > ### 1. Base Model 选择
  >
  > BERTSUM
  >
  > * 模型效果
  >
  >   BERTSUM包括生成式BertSumAbs、抽取式BertSumExt、混合BertSumExtAbs 3种方法。
  >
  >   BERTSUM paper在 CNN/DailyMail、NYT 和XSum三个数据集下进行了实验。结果显示，三种BERTSUM在ROUGE、Position of Extracted Sentences 、Novel N-grams 、Human Evaluation（QA） 都完胜之前的模型。
  >
  >   Results on CNN/DailyMail (20/8/2019):
  >
  >   <table class="tg">
  >     <tr>
  >       <th class="tg-0pky">Models</th>
  >       <th class="tg-0pky">ROUGE-1</th>
  >       <th class="tg-0pky">ROUGE-2</th>
  >       <th class="tg-0pky">ROUGE-L</th>
  >     </tr>
  >     <tr>
  >       <td class="tg-0pky">Oracle</td>
  >       <td class="tg-0pky">52.59</td>
  >       <td class="tg-0pky">31.24</td>
  >       <td class="tg-0pky">48.87</td>
  >     </tr>
  >     <tr>
  >       <td class="tg-c3ow" colspan="4">Extractive</td>
  >     </tr>
  >     <tr>
  >       <td class="tg-0pky">TransformerExt</td>
  >       <td class="tg-0pky">40.90</td>
  >       <td class="tg-0pky">18.02</td>
  >       <td class="tg-0pky">37.17</td>
  >     </tr>
  >     <tr>
  >       <td class="tg-0pky">BertSumExt</td>
  >       <td class="tg-0pky">43.23</td>
  >       <td class="tg-0pky">20.24</td>
  >       <td class="tg-0pky">39.63</td>
  >     </tr>
  >     <tr>
  >       <td class="tg-0pky">BertSumExt (large)</td>
  >       <td class="tg-0pky">43.85</td>
  >       <td class="tg-0pky">20.34</td>
  >       <td class="tg-0pky">39.90</td>
  >     </tr>
  >     <tr>
  >       <td class="tg-baqh" colspan="4">Abstractive</td>
  >     </tr>
  >     <tr>
  >       <td class="tg-0lax">TransformerAbs</td>
  >       <td class="tg-0lax">40.21</td>
  >       <td class="tg-0lax">17.76</td>
  >       <td class="tg-0lax">37.09</td>
  >     </tr>
  >     <tr>
  >       <td class="tg-0lax">BertSumAbs</td>
  >       <td class="tg-0lax">41.72</td>
  >       <td class="tg-0lax">19.39</td>
  >       <td class="tg-0lax">38.76</td>
  >     </tr>
  >     <tr>
  >       <td class="tg-0lax">BertSumExtAbs</td>
  >       <td class="tg-0lax">42.13</td>
  >       <td class="tg-0lax">19.60</td>
  >       <td class="tg-0lax">39.18</td>
  >     </tr>
  >   </table>
  >
  > * 模型迁移到NLPCC 2017数据集的可能性
  >
  >   ![](https://i.imgur.com/3GT6sVo.jpg)
  >
  >   NLPCC 2017数据集分析：
  >
  >   > 数据集有50000条样本，来源于今日头条各类新闻；
  >   >
  >   > 文章长度在700字左右；
  >   >
  >   > 摘要多数为1句，平均字数为45字；
  >   >
  >   > 相比于英文数据集，摘要中新词比例平均为10%，明显偏少。
  >
  >   |               | mean | min  | 25%  | median | 75%  |  max  | mode |
  >   | :-----------: | :--: | :--: | :--: | :----: | :--: | :---: | :--: |
  >   |   文章字数    | 1036 | 101  | 392  |  719   | 1293 | 40488 | 181  |
  >   |   文章句数    |  22  |  1   |  8   |   15   |  29  |  579  |  6   |
  >   |   摘要字数    |  45  |  21  |  40  |   47   |  51  |  128  |  50  |
  >   |   摘要句数    | 1.4  |  1   |  1   |   1    |  2   |   6   |  1   |
  >   | 摘要新字（%） |  9   |  0   |  3   |   7    |  13  |  96   |  0   |
  >   | 摘要新词（%） |  13  |  0   |  6   |   12   |  18  |  60   |  8   |
  >
  > 
  >
  > ### 2. 数据预处理
  >
  > 本文BertSum[2]采取与Neusum[3]类似的方法生成Oracle，代码入口为process.py。
  >
  > >  We use an unsupervised approach to convert the abstractive summaries to extractive labels. Our approach is based on the idea that the selected sentences from the document should be the ones that maximize the Rouge score with respect to gold summaries. Since it is computationally expensive to find a globally optimal subset of sentences that maximizes the Rouge score, we employ a greedy approach, where we add one sentence at a time incrementally to the summary, such that the Rouge score of the current set of selected sentences is maximized with respect to the entire gold summary . We stop when none of the remaining candidate sentences improves the Rouge score upon addition to the current summary set. We return this subset of sentences as the extractive ground-truth, which is used to train our RNN based sequence classifier. 
  >
  > 预处理的方式参考https://github.com/nlpyang/PreSumm和https://github.com/magic282/cnndm_acl18，两者在生成oracle上略有不同。
  >
  > 本项目采用rouge_score = rouge1_f1+rouge2_f1为指标，设置oracle上限为3，利用贪心的方式生成oracle。
  >
  > Oracle的rouge就是抽取式摘要的上限，可以看到中文数据集与CNN/DailyMail数据集在这个指标上是类似的。
  >
  > |                  | mean  |  min  |  25%  | median |  75%  | max  | mode |
  > | ---------------- | :---: | :---: | :---: | :----: | :---: | :--: | :--: |
  > | Oracle rouge1_f1 | 53.76 | 24.99 | 44.44 | 52.63  | 62.12 | 0.99 |  0   |
  > | Oracle rouge2_f1 | 35.00 | 0.00  | 23.08 | 32.00  | 44.19 | 0.99 |  0   |
  >
  > 除此之外，本项目对原文和摘要的长度也作出了一定的限制。
  >
  > ```python
  > parser.add_argument('-max_src_ntokens', default=512, type=int, help='原文中最多的字数')
  > parser.add_argument('-min_src_nsents', default=3, type=int, help='原文中最少的句子数')
  > parser.add_argument('-max_src_nsents', default=100, type=int, help='原文中最多的句子数')
  > parser.add_argument('-min_src_ntokens_per_sent', default=5, type=int, help='原文中每个句子最少字数')
  > parser.add_argument('-max_src_ntokens_per_sent', default=200, type=int, help='原文中每个句子最多字数')
  > parser.add_argument('-min_tgt_ntokens', default=10, type=int, help='摘要中最少的字数')
  > parser.add_argument('-max_tgt_ntokens', default=150, type=int, help='摘要中最多的字数')
  > ```
  >
  > 最终得到48674个样本（train :38939 and test :9735）。
  >
  > 每条样本格式为：
  >
  > ```
  > {"src": [原文对应词表的序号], 
  >  "tgt": [oracle对应词表的序号],
  >  "src_sent_labels": [oracle位置为1], 
  >  "segs": [seg_ids], 
  >  'clss': [cls_ids],
  > 'src_txt': [src_txt], 
  > "tgt_txt": [tgt_txt]}      
  > ```
  >
  > 
  >
  > ### 3. 模型
  >
  > ![](https://i.imgur.com/f5WFbX3.jpg)
  >
  > 
  >
  > ### 4.实验结果
  >
  > [1] Text Summarization with Pretrained Encoders 
  >
  > [2] SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents 

* 准备开题报告

  基于图模型的智能数据库问答系统

* 论文阅读





# 论文阅读

**目录**

|                           文章标题                           |      主题      |    来源    | 特点                                                         |
| :----------------------------------------------------------: | :------------: | :--------: | :----------------------------------------------------------- |
|         Text Summarization with Pretrained Encoders          |      摘要      | EMNLP2019  | 根据摘要任务改进bert，效果好（SOTA），实验充足               |
| SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents |      摘要      | AAAI-2017  | 虽然效果不够好，但是文中提出的2个创新点（Abstractive Training、classification）都是很有启发性的。对于生成式摘要，到底什么是选取一个句子的根本依据？对于joint extractive-abstractive model，如何连接这2个模块？ |
|   RoBERTa: A Robustly Optimized BERT Pretraining Approach    |   预训练模型   |  Facebook  | 英文原版：https://github.com/pytorch/fairseq                                                                                                     ai-indeed中文版：https://github.com/brightmart/roberta_zh                                                         哈工大讯飞联合实验室中文版：https://github.com/ymcui/Chinese-BERT-wwm |
| Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | Language Model |   Google   | 针对Transformer对长文本编码效果差的问题，提出了Transformer-XL。相比于Transformer，它提出了segment-level recurrence mechanism和Relative Positional Encodings，能有效解决编码长距离依赖和context fragmentation。效果上，Transformer-XL学到的依赖要比 RNN 学到的长 80%，比最初的 Transformer 网络长 450%。所以在XLNet中也应用了Transformer-XL。 |
| Global Reasoning over Database Structures for Text-to-SQL Parsing |   GNN NL2SQL   | EMNLP 2019 | 2个创新点：在encoder  GCN 中加入了Global Reasoning；用了re-ranking GCN去替代Execution-Guided Decoding。 |

 

## **1. Text Summarization with Pretrained Encoders**  

EMNLP 2019

* 关键词

  Document-level Encoder ,  Extractive Summarization, Abstractive Summarization,  Two-staged fine-tuning approach 

* 方法

  * Extractive Summarization 

    ![](https://i.imgur.com/IEg13HT.jpg)

    * Several inter-sentence Transformer layers (L=2) are then stacked on top of BERT outputs, to capture document-level features for extracting summaries .

    * 首先获得每个句子的得分，然后选出得分最高的3个作为摘要。

    * 为了减少挑选句子的信息冗余，采用了*Trigram Blocking*，即当已有摘要与候选句子中有3个词的重复时，就忽略候选句子。

    * 学习率的设置：

    ![](https://i.imgur.com/opP4tQb.jpg)

  * Abstractive Summarization 

    * encoder-decoder framework

    * The encoder is the pretrained BERTSUM and the decoder is a 6-layered Transformer initialized randomly. 

      自然encoder and the decoder 之间存在mismatch。所以Abstractive的encoder、decoder用了不同学习率的Adam optimizers 。

    * <u>Two-staged fine-tuning approach</u>：we first fine-tune the encoder on the extractive summarization task and then fine-tune it on the abstractive summarization task 

    * decoder采用beam search、对长度进行惩罚、end-of-sequence token 

*  数据集

  * CNN/DailyMail 

    gold summary 更偏向于抽取

  * NYT

    实验用的长度50-800，gold summary 更偏向于抽取

  * XSum 

    实验用的长度-512，gold summary 更偏向于生成

  * 产生抽取式摘要gold的方法与之前的neusum一样，都是参考了Nallapati et al. (2017)  

    The algorithm generates an oracle consisting of multiple sentences which maximize the ROUGE-2 score against the gold summary. 

* 实验结果

  * ROUGE

    ROUGE-1、ROUGE-2（信息丰富度量） ；ROUGE-L（流畅度）

    不同数据集，BERTSUMABS、BERTSUMEXT表现各有优劣，不过都比之前的方法好。

  * Learning Rates 

    验证生成式模型的encoder、decoder设置不同学习率时，模型的perplexity 。

    > 句子概率越大，语言模型越好，perplexity 越小

  * Position of Extracted Sentences 

  * Novel N-grams 

    BERTEXTABS is more biased towards selecting sentences from the source document since it is initially trained as an extractive model. 

  * Human Evaluation 

* 评价

  这篇文章是作者在他5月的那篇论文上进一步深入得到的结果。5月的那篇论文是基于抽取式摘要的特点，在bert上改进了一点点，得到了不错的效果，也证明了预训练模型在文摘任务上的可行性。

  进一步，作者加上了decoder部分，模型就能用在生成摘要任务上了。添加decoder时，作者有思考如何利用5月份的结果（抽取式摘要），这也是为什么作者能提出Two-staged fine-tuning approach和a new fine-tuning schedule。

  优点：模型在bert的基础上改进，所以虽然改动小，但是效果好；实验详细，从多个角度上验证了模型的效果，并且文中提出的每一个点后面都有好好做了实验。

  缺点：论文创新性不够，模型效果好很大程度是因为bert。

  

## 2. SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents 

AAAI-2017

* 关键词

   Extractive Summarization, Training Mechanism, Interpretable 

* 方法

  * SummaRuNNer 

    ![](https://i.imgur.com/5eIt8aZ.jpg)

    1. 采用2层的GRU对sentence进行encoder（这种方法现在已是常规操作）

    2. ==classification== 

       作分类任务时，有考虑到句子本身表达、句子与全文的关系、句子与已生成摘要部分的关系（去重）、句子的绝对位置、句子的相对位置（句子位于哪个段落）信息。

       ![](https://i.imgur.com/46VA3L8.jpg)

       > 我觉得这个想法很棒呀，但是后面的论文中都没再见过这样的分类方法…
       >
       > 句子本身表达：这个一定有用到
       >
       > 句子与全文的关系：没再见到过。可以认为bert中attention含有这个功能
       >
       > 句子与已生成摘要部分的关系：放在了后处理中
       >
       > 句子的绝对位置、句子的相对位置：没再出现过。但是结果显示很多模型能学到这种位置关系
       >
       > 可能是想法很好，但是很鸡肋的点吧(￣(工)￣)

    3. loss function 

       negative log-likelihood （常规操作）

       ![](https://i.imgur.com/KLw9Q4i.jpg)

  * Training 

    * Extractive Training （常规方式）

      利用无监督的方法将生成式摘要转化成抽取式的labels

      想法是从文章中挑选的句子应该是相对于gold summaries，能使rouge score最大化的那个句子。采用贪心算法，每次增加一个句子能使oracle的rouge相对于gold summaries最大。当增加句子不能提高rouge score时就停止添加句子。这个句子集合就被称为oracle，也就是训练的label。

    * ==Abstractive Training== 

      出发点是想把生成oracle这一步省去，毕竟oracle只是一种近似。

      方法就是在训练过程中增加一个decoder，强行在抽取式的过程中加入生成式。

      整体上是RNN结构，红色的是GRU单元，黄色是一个前馈神经层f，最后接softmax层，输出概率$p_k$，$p_k$是一个维数与词汇数量相同的向量，每一维表示某个词的概率。每一个时间步骤根据输出的$p_k$得到对应的词，再将该词的词向量$x_k$作为下一时间步骤的输入，同时由SummaRuNNer最后一个时间步骤得到的文摘表示$s_{-1}$ 作为decoder的每个时间步骤的输入。

      ![](https://i.imgur.com/m3oacwb.png)

      GRU模块：

      ![](https://i.imgur.com/VMJtIM5.jpg)

      feed-forward layer f and softmax：

      ![](https://i.imgur.com/60biQNI.jpg)

      Loss function：

      希望gold summary中出现的词概率之积最大

      ![abstractiveTraining-loss](https://img-blog.csdn.net/20171130171006904?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjUyMjIzNjE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

      在测试的时候，不会用decoder部分

      > since the summary representation s−1 acts as the only information channel between the SummaRuNNer model and the decoder, maximizing the probability of abstractive summary words as computed by the decoder will require the model to learn a good summary representation which in turn depends on accurate estimates of extractive probabilities p(yj). 
      >
      > 只有得到一个非常好的抽取式摘要平均表达$s_{-1}$，才能使得decoder中得到最大的the probability of abstractive summary words；反之也成立。
      >
      > 道理是这个道理，但是效果不见得会很好，目测不如直接训练(￣◇￣;)	



# 3. RoBERTa: A Robustly Optimized BERT Pretraining Approach 

Facebook

* 关键词

  BERT, Training strategies 

* 方法

  对于BERT pretraining而言，超参和training set size的选择非常重要，但是庞大的参数也给调参带来了巨大的困难， 我们往往没有足够的时间为一个模型找到最佳参数。所以Facebook这篇文章的贡献在于修改了BERT的训练方式——RoBERTa，以期将BERT的效果再次提到SOTA。（意思就是XLNet【autoregressive language model】花里胡哨，我们把BERT【masked language model 】好好训练下，还能吊打你们，23333）。

  **RoBERTa is trained with dynamic masking , FULL-SENTENCES without NSP loss , large mini-batches and a larger byte-level BPE.**

  修改的点：

  1. training the model longer, with bigger batches, over more data; 

     > 原BERT使用了bs=256和1M步的训练方法，这等价于bs=2K和125K步或者bs=8K和31K步，后2者效果更好
     >
     > 更大的byte-level BPE

  2. removing the next sentence prediction objective;

     > 通过4项实验，证明了DOC-SENTENCES（去掉NSP）效果最好

  3. training on longer sequences;

  4.  dynamically changing the masking pattern applied to the training data. 

     > Masked Language Model任务
     >
     > Static Masking 是指只在预处理时做一次mask操作，在训练过程中保持不变。
     >
     > 自然，Dynamic Masking 是指在trian过程中，不同epoch做的mask操作不同，那么避免样本在每个epoch中mask一样。

  还有一点就是用了更大的新数据集CC-NEWS

* 实验结果

  ![](https://i.imgur.com/obkt2Vl.jpg)

  ​	![](https://i.imgur.com/bQhDEfv.jpg)

  ![](https://i.imgur.com/c9y1kcw.jpg)

# 4.Character-Level Language Modeling with Deeper Self-Attention 

AAAI-19

* 关键词

  character-level language modeling,  deep  transformer model 

* 方法

  普通RNN用于character-level language model：
  将句子按character为单位组成多个batch，每个batch预测最后一个词，然后将该batch的隐状态传入下一个batch。也即“truncated backpropagation through time” (TBTT)。

  虽然这样的结构取到了很好的成效，但是它增加了训练的复杂性。但最近一些研究表明，用这样的方式训练模型实际上并没有真的得到‘很强’的长期记忆能力。

  Character Transformer Model ：

  使用64层模型，并仅限于处理 512个字符这种相对较短的输入，因此它将输入分成段，并分别从每个段中进行学习。 在测试阶段如需处理较长的输入，该模型会在每一步中将输入向右移动一个字符，以此实现对单个字符的预测。

  * Auxiliary Losses 
  * Positional Embeddings 

  

# 5. Transformer-XL

ACL 2019 , Google 

* 关键词

  language model, longer-term dependency 

* 方法

  * Introduction 

    Transformer编码固定长度的上下文，即将一个长的文本序列截断为几百个字符的固定长度片段(segment)，然后分别编码每个片段，片段之间没有任何的信息交互。比如BERT，序列长度的极限一般在512。

    - Transformer无法建模超过固定长度的依赖关系，对长文本编码效果差。
    - Transformer把要处理的文本分割成等长的片段，通常不考虑句子（语义）边界，导致**上下文碎片化(context fragmentation)**。通俗来讲，一个完整的句子在分割后，一半在前面的片段，一半在后面的片段。

    文章围绕如何建模长距离依赖，提出Transformer-XL【XL是extra long的意思】。

  * Model

    为了将Transformer或self-attention应用到语言建模中，核心问题是如何训练Transformer有效地将**任意长的上下文**编码为**固定大小的表示**。

    > 之前的尝试：
    >
    > 尝试1:  如果能给定无限内存和计算，那么最简单的解决方案是使用unconditional  Transformer decoder处理整个上下文序列，类似于前馈神经网络。然而，在实践中，由于资源有限，这通常是不可行的。
    >
    > 尝试2:（vanilla model by Al-Rfou et al. (2018). ）将**整个语料库**分割成**固定大小**的**片段**，只在**每个片段中训练模型**，**忽略**来自**前一段的所有上下文信息**。这造成了**三个关键的限制**：
    >
    > ![](https://i.imgur.com/GkPb7Eo.jpg)
    >
    > - 字符之间的最大依赖距离受输入长度的限制，模型看不到出现在几个句子之前的单词。
    > - 对于长度超过512个字符的文本，都是从头开始单独训练的。段与段之间没有上下文依赖性，会让训练效率低下，也会影响模型的性能。
    > - 在测试阶段，每次预测下一个单词，都需要重新构建一遍上下文，并从头开始计算，这样的计算速度非常慢。

    所以就有了本文的Transformer-XL。

    * Segment-Level Recurrence with State Reuse 

      Transformer-XL仍然是使用分段的方式进行建模，但其与Vanilla Transformer的本质不同是在于引入了段与段之间的循环机制，使得当前段在建模的时候能够利用之前段的信息来实现长期依赖性。如下图所示：

      ![](https://i.imgur.com/0tQjeJK.jpg)

      在训练阶段，处理后面的段时，每个隐藏层都会接收两个输入：

      1. 该段的前面节点的输出，与Vanilla Transformer相同（上图的灰色线）。
      2. 前面段的节点的输出（上图的绿色线），可以使模型创建长期依赖关系。这部分输出通过cache的机制传导过来，所以不会参与梯度的计算。原则上只要GPU内存允许，该方法可以利用前面更多段的信息。

      计算过程如下：对于两个连续的分割片段 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bs%7D_%7B%5Ctau%7D%3D%5Cleft%5Bx_%7B%5Ctau%2C+1%7D%2C+%5Ccdots%2C+x_%7B%5Ctau%2C+L%7D%5Cright%5D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bs%7D_%7B%5Ctau%2B1%7D%3D%5Cleft%5Bx_%7B%5Ctau%2B1%2C1%7D%2C+%5Ccdots%2C+x_%7B%5Ctau%2B1%2C+L%7D%5Cright%5D) ，第n层的隐状态计算公式如下：

      ![[公式]](https://www.zhihu.com/equation?tex=%5Cwidetilde%7B%5Cmathbf%7Bh%7D%7D_%7B%5Ctau%2B1%7D%5E%7Bn-1%7D%3D%5Cleft%5B%5Cmathrm%7BSG%7D%5Cleft%28%5Cmathbf%7Bh%7D_%7B%5Ctau%7D%5E%7Bn-1%7D%5Cright%29+%5Ccirc+%5Cmathbf%7Bh%7D_%7B%5Ctau%2B1%7D%5E%7Bn-1%7D%5Cright%5D%5C%5C)
      ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bq%7D_%7B%5Ctau%2B1%7D%5E%7Bn%7D%2C+%5Cmathbf%7Bk%7D_%7B%5Ctau%2B1%7D%5E%7Bn%7D%2C+%5Cmathbf%7Bv%7D_%7B%5Ctau%2B1%7D%5E%7Bn%7D%3D%5Cmathbf%7Bh%7D_%7B%5Ctau%2B1%7D%5E%7Bn-1%7D+%5Cmathbf%7BW%7D_%7Bq%7D%5E%7B%5Ctop%7D%2C+%5Ctilde%7B%5Cmathbf%7Bh%7D%7D_%7B%5Ctau%2B1%7D%5E%7Bn-1%7D+%5Cmathbf%7BW%7D_%7Bk%7D%5E%7B%5Ctop%7D%2C+%5Cwidetilde%7B%5Cmathbf%7Bh%7D%7D_%7B%5Ctau%2B1%7D%5E%7Bn-1%7D+%5Cmathbf%7BW%7D_%7Bv%7D%5E%7B%5Ctop%7D%5C%5C)
      ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bh%7D_%7B%5Ctau%2B1%7D%5E%7Bn%7D%3D%5Ctext+%7B+Transformer-Layer+%7D%5Cleft%28%5Cmathbf%7Bq%7D_%7B%5Ctau%2B1%7D%5E%7Bn%7D%2C+%5Cmathbf%7Bk%7D_%7B%5Ctau%2B1%7D%5E%7Bn%7D%2C+%5Cmathbf%7Bv%7D_%7B%5Ctau%2B1%7D%5E%7Bn%7D%5Cright%29%5C%5C)
      ![[公式]](https://www.zhihu.com/equation?tex=SG%28%5Ccdot%29) 函数表示的是梯度截断或者梯度停止(stop-gradient)， ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5B%5Cmathbf%7Bh%7D_%7Bu%7D+%5Ccirc+%5Cmathbf%7Bh%7D_%7Bv%7D%5Cright%5D) 表示两者拼接。

      这个额外的连接**将最大可能依赖关系长度增加了N倍**，其中N表示网络的深度，因为上下文信息现在可以跨片段边界流动。

      在预测阶段：

      如果预测$x_{11}$我们只要拿之前预测好的$[x_1,x_2,...,x_{10}]$的结果拿过来，直接预测。同理在预测$x_{12}$的时候，直接在$[x_1,x_2,...,x_{10},x_{11}]$的基础上计算，不用像Vanilla Transformer一样每次预测一个字就要重新计算前面固定个数的词。

      > For vanilla Transformer language models (Al Rfou et al), you process [1 2 3 4], predict 5, process [2 3 4 5], predict 6, and repeat.
      > For a Transformer-XL language model, you process [1 2 3 4], predict [5 6 7 8], predict [9 10 11 12], and repeat. Note that we don't have to reprocess [5 6 7 8] when predicting [9 10 11 12], because the representations can be reused.

    * Relative Positional Encodings 

      https://www.cnblogs.com/shiyublog/p/11236212.html

      `Segment-level recurrence mechanism`机制中提到的，`max_len`窗口中的每个token都能attention前`max_len`个token，其中可能会有一些token在上一个seg，这样就存在位置编码不连续，或者相同token在当前seg和前一个seg具有相同的attention值的问题。例如，考虑一个具有上下文位置[0,1,2,3]的旧段。当处理一个新的段时，我们将两个段合并，得到位置[0,1,2,3,0,1,2,3]，其中每个位置id的语义在整个序列中是不连贯的。

      为此，我们提出Relative Positional Encodings。

      ![[公式]](https://www.zhihu.com/equation?tex=E%2CU%2CR%2CW) 分别表示`token emb, absolute pos emb, relative pos emb, proj matrix`。

      那么，绝对位置编码公式为：

      ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BA%7D_%7Bi%2C+j%7D%5E%7B%5Cmathrm%7Babs%7D%7D%3Dq_%7Bi%7D%5E%7B%5Ctop%7D+k_%7Bj%7D%3D%5Cunderbrace%7B%5Cmathbf%7BE%7D_%7Bx_%7Bi%7D%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bq%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bk%7D+%5Cmathbf%7BE%7D_%7Bx_%7Bj%7D%7D%7D_%7B%28a%29%7D%2B%5Cunderbrace%7B%5Cmathbf%7BE%7D_%7Bx_%7Bi%7D%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bq%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bk%7D+%5Cmathbf%7BU%7D_%7Bj%7D%7D_%7B%28b%29%7D%2B%5Cunderbrace%7B%5Cmathbf%7BU%7D_%7Bi%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bq%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bk%7D+%5Cmathbf%7BE%7D_%7Bx_%7Bj%7D%7D%7D_%7B%28c%29%7D%2B%5Cunderbrace%7B%5Cmathbf%7BU%7D_%7Bi%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bq%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bk%7D+%5Cmathbf%7BU%7D_%7Bj%7D%7D_%7B%28d%29%7D%5C%5C)
      相对位置编码公式为：

      ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BA%7D_%7Bi%2C+j%7D%5E%7B%5Cmathrm%7Brel%7D%7D%3D%5Cunderbrace%7B%5Cmathbf%7BE%7D_%7Bx_%7Bi%7D%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bq%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bk%2C+E%7D+%5Cmathbf%7BE%7D_%7Bx_%7Bj%7D%7D%7D_%7B%28a%29%7D%2B%5Cunderbrace%7B%5Cmathbf%7BE%7D_%7Bx_%7Bi%7D%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bq%7D%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bk%2C+R%7D+%5Cmathbf%7BR%7D_%7Bi-j%7D%7D_%7B%28b%29%7D%2B%5Cunderbrace%7Bu%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bk%2C+E%7D+%5Cmathbf%7BE%7D_%7Bx_%7Bj%7D%7D%7D_%7B%28c%29%7D%2B%5Cunderbrace%7Bv%5E%7B%5Ctop%7D+%5Cmathbf%7BW%7D_%7Bk%2C+R%7D+%5Cmathbf%7BR%7D_%7Bi-j%7D%7D_%7B%28d%29%7D%5C%5C)

      > ![](https://i.imgur.com/3mmweww.jpg)

      绝对位置编码在输入transformer之前就和`token emb`求和，相对位置编码需要在计算`attention score`加入和计算。

    小结一下，**segment-level recurrence mechanism**为了解决编码长距离依赖和context fragmentation，**relative position embedding scheme**为了实现segment-level recurrence mechanism而提出，解决可能出现的时序混淆问题。

    Transformer-XL architecture：

    A N-layer Transformer-XL with a single attention head 

    ![](https://i.imgur.com/f1kOdV5.jpg)



# 6. Global Reasoning over Database Structures for Text-to-SQL Parsing

* Keywords

  Global Reasoning ；Re-ranking GCN

* 方法

  NL2SQL的难点在于如何把训练阶段没有见过的lexical items对应到DB上。之前的方法是求words和DB constants 之间的 *local* similarity function ，这个过程与auto-regressive decoder 结合起来，这个过程会忽略掉全局信息。所以我们需要在这个过程中加入全局的信息。

  * Schema-augmented Semantic Parser 

    * 问题构建：

      训练集 $\{(x^k,y^k,S^k)\}^N_{k=1}$，其中$x^k$是question，$y^k$是SQL query，$S^k$是DB schema，S包括DB tables、a set of columns for each table、a set of foreign key-primary key column pairs。

      我们的目标就是训练model，输入question-schema pairs (x, S) 能得到正确的SQL query ，特别是针对从未训练过的样本。

    * Base model

      用BiLSTM对input question $(x_1, . . . , x_{|x|}) $做Encoder，而output query $y$ 用另一个结合了SQL grammar 的LSTM网络进行decoder。我们的核心关注点是decoder的过程。

      > 毕设想改的就是encoder的过程，decoder可以用单表查询的方式。
      >
      > 补充说明下：encoder是对question作表征，希望能找到question与column name的联系；decoder是结合SQL的语法、question hidden states与DB schema得到的top-down结果。

      在decoder的过程中，为了选择DB constant，我们首先要求 DB constant $v$ 对question words的attention distribution$\{a_i\}^x_{i=1}$，然后求 $s_v=\sum_ia_is_{link}(v,x_i)$，这里的$s_{link}$是 local similarity score（人工特征）。最后再做$softmax（\{s_v\}_{v∈V}） $即可。但是，这个decoder过程并没有很好地结合DB constants，意思就是$s_{link}$选的不好。

    * DB schema encoding 

      把 DB schema与GNN结合起来：nodes是DB constants ，edges是连接tables and their columns 。

      > 这个过程详见：Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing

      GCN的输入$h_v^0=ρ_v \cdot r_v$，$ρ_v$ is relevance probability which can be viewed as a soft selection for whether the DB constant should appear in the output 。$ρ_v$的计算过程并没有考虑到full question or DB structure，所以这个就是本文改进的点。

  * Global Reasoning over DB Structures 

    ![](https://i.imgur.com/qmTexZi.jpg)

    * Global gating  

      目的是训练GCN直接由 global context of the question and DB去预测$ρ_v$。

       GGCN的输入新增了a new node $v_{global}$，这个node通过特殊的edge连接到all other nodes。
      $$
      g_v^0=FF([r_v;\overline{h_v};ρ_v]) \\
      \overline{h_v}= \sum_ip_{link}(x_i|v) \cdot e_i\\
      ρ_v^{global}=σ (FF(g_v^L))
      $$
      这个$ρ_v^{global}$将会取代encoder GCN 中的$ρ_v$。

      同时，我们也会在loss中加入*relevance loss* ：

      ![](https://i.imgur.com/OMu9vsx.jpg)

      $\mathcal{U}_\hat {y} $:the gold subset of DB constants 

    * Discriminative re-ranking 

      we separately train  discriminative model to re-rank the top-K queries in the decoder’s output beam.

      > 就是替代Execution-Guided Decoding 

      ![](https://i.imgur.com/2sP7itu.jpg)

      对于每一个candidate，我们会计算：

      ![](https://i.imgur.com/16h6Rlj.jpg)

      其中：

      $w$是 a learned parameter vector；$f_{\mathcal{U}_\hat {y} }$是$\mathcal{U}_\hat {y} $的representation；$e^{align}$is a representation for the global alignment between question words and DB constants. 

      *re-ranker loss* ：the negative log probability of the correct query y

      我们会基于re-ranking GCN来设计 $\mathcal{U}_\hat {y}$和$e^{align} $。

      re-ranking GCN：

      re-ranking GCN将the selected DB constants $\mathcal{U}_\hat {y}$和 the global node $v_{global}$作为输入。

      $f_{\mathcal{U}_\hat {y}}$ captures global properties of selected nodes but ignores nodes that were not selected and are possibly relevant.  

      > $f_v^0=FF(r_v;\overline{h_v})\\ f_{\mathcal{U}_\hat {y}}=f^{L_{v_{global}}}$

      $e^{align}$captures whether question words are aligned to selected DB constant .

      > ![](https://i.imgur.com/86ik2Fl.jpg)
      >
      > $l^φ _i=\sum_vp_{link}(v|x_i)\cdot φ_v$
      >
      > $e_i^{align}=[e_i;l^φ _i]$

  * Results 

    ![](https://i.imgur.com/8vBsG6l.jpg)







