# 20180816学习总结

# 本周工作内容

## 1. 实现了Ensemble模型

* Ensemble策略：stacking

* stacking 第一层

  * TextCNN，LSTM

  * Input ：微博文本的词向量Word2vec

* Stacking 第二层：XGBoost

  * Input ： 第一层的每个算法通过3折交叉验证后得到的3个模型，在验证集（3次）与测试集上得到的预测结果，构成第二层train set与test set</br>

    </n>

## 2. XGBoost

* 原理
  * XGBoost采用的是基本模型是决策树，串行增加结点，每次增加新结点的策略是：加入的新结点要能够使整体的性能（OBj）有提升 boosting
  * 具体可以看：
    1. [Introduction to Boosted Trees](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
    2. [XGBoost: A Scalable Tree Boosting](http://delivery.acm.org/10.1145/2940000/2939785/p785-chen.pdf?ip=202.120.234.63&id=2939785&acc=CHORUS&key=BF85BBA5741FDC6E%2E88014DC677A1F2C3%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1534405887_82ace399754b0674bb6948aec5a83695)</br>
* xgboost library
  * [xgboost官网](https://xgboost.readthedocs.io/en/latest/index.html#)
  * [参数说明](https://xgboost.readthedocs.io/en/latest/parameter.html)
  * [python API](http://xgboost.readthedocs.io/en/latest/python/python_api.html)
  * [官方demo](https://github.com/dmlc/xgboost/tree/master/demo/guide-python)

</n>

## 3. Ensemble Learning

* 综述

  * [Ensemble Learning](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)

* 常见的三种方法

  * Bagging

    可以减少方差

    随机森林

  * Boosting

    Boosting可以减少偏差bias，缓解过拟合

    Adaboost 

  * Stacking

    [Stacking](https://zhuanlan.zhihu.com/p/26890738)

    </n> 



# 下周计划

* 多任务学习
* 把多个特征加入到训练中

