# 1. 本周学习内容

1. 完成专利

2. NL2SQL

   * 复现Global-GNN base model

     * AllenNLP：基于pytorch的NLP深度学习框架

       https://zhuanlan.zhihu.com/p/83115879

       https://www.twblogs.net/a/5d07a90bbd9eee1ede03acc3/zh-cn

     * Code框架

       ![](https://i.imgur.com/XgZL0Ip.jpg)

     * 实验结果

       比官方结果差了0.001

       ```
         "best_validation__match/exact_match": 0.3656862745098039,
         "best_validation_sql_match": 0.492156862745098,
         "best_validation__others/action_similarity": 0.5755201074796751,
         "best_validation__match/match_single": 0.6328413284132841,
         "best_validation__match/match_hard": 0.33263598326359833,
         "best_validation_beam_hit": 0.614313725490196,
         "best_validation_loss": 8.321221351623535
       ```

   * 结合pretrain LM

3. GNN汇报

4. 论文阅读

   |                           文章标题                           |  主题  |    来源     | 特点                                                         |
   | :----------------------------------------------------------: | :----: | :---------: | :----------------------------------------------------------- |
   | Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions | NL2SQL | EMNLP 2019  | 多轮对话+NL2SQL                                              |
   | ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS | ALBERT | Google 2019 | 轻量级。提出了Factorized embedding parameterization和Cross-layer parameter sharing方法来减少参数量；还改良了NSP-SOP loss |

   

# 2. 下周工作内容

1. NL2SQL

   * BERT

   * bad case分析→改进seq2seq方法

2. 文本摘要
   * 结果分析



# 3. 论文阅读

## 1. Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions 

* Keywords

  Cross-Domain；Multi-turn interaction

* 方法

  相邻的Question往往在逻辑上有依赖关系，所以他们相应的SQL queries也会有重叠部分，我们可以利用之前预测query的历史记录来提高生成的质量。

  > We propose query generation by editing the query in the previous turn. To this end, we first encode the previous query as a sequence of tokens, and the decoder computes a switch to change it at the token level. This sequence editing mechanism models token-level changes and is thus robust to error propagation. Furthermore, to capture the user ut-terance and the complex database schemas in different domains, we use an utterance-table encoder based on BERT to jointly encode the user utterance and column headers with co-attention, and adopt a table-aware decoder to perform SQL generation with attentions over both the user utterance and column headers. 

  ![](https://i.imgur.com/mFGGamb.jpg)

## 2. ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS

* Keywords

* 方法

  * 渴望轻量级网络

    * The memory limitations of available hardware 

    * Training speed 

    * Simply growing the hidden size of a model such as BERT-large can lead to worse performance 

      ![](https://i.imgur.com/OX924lv.jpg)

      > For example, Devlin et al. (2019) show that across three selected natural language understanding tasks, using larger hidden size, more hidden layers, and more attention heads always leads to better performance. <u>However, they stop at a hidden size of 1024. We show that, under the same setting, increasing the hidden size to 2048 leads to model degradation and hence worse performance. Therefore, scaling up representation learning for natural language is not as easy as simply increasing model size.</u> 

  * Existing solutions 
    * model parallelization 
    * clever memory management 

  * ALBERT 

    Two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT 

    * ==A factorized embedding parameterization== 

      By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding.  This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings.

      > 对于 Bert，词向量维度 E 和隐层维度 H 是相等的。在 large 和 xlarge 等更大规模的模型中，E 会随着 H 不断增加。作者认为没有这个必要，因为预训练模型主要的捕获目标是 H 所代表的“上下文相关信息”而不是 E 所代表的“上下文无关信息”。
      >
      > 说白了就是在词表 one-hot vectors V 到 隐层 H 的中间，插入一个小维度 E，多做一次尺度变换：O(V × H) to O(V × E + E × H) （ H ≫ E）
      >
      > We first project  one-hot vectors  into a lower dimensional embedding space of size E, and then project it to the hidden space.
      >
      > 一个VxH的embedding变成两个VxE, ExH的两个fc

    *  ==cross-layer parameter sharing==

      This technique prevents the parameter from growing with the depth of the network. 

      > There are multiple ways to share parameters:
      >
      > 1. only sharing feed-forward network (FFN) parameters across layers 
      >
      > 2. only sharing attention parameters
      > 3. sharing all parameters

    * Inter-sentence coherence loss 

      ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. 

      > SOP目标补偿了一部分因为embedding和FFN共享而损失的性能。BERT原版的NSP目标过于简单，它把*topic prediction* 和 *coherence prediction* 放在一个任务中实现。SOP对其加强，它的正样本与原版BERT一样（一篇文章中连贯的2个段落），它的负样本换成了同一篇文章中的2个逆序的句子，这迫使模型去学习语句连贯性的细粒度区别，从而消去了topic prediction。

    * Removing dropout

    * 1. 模型的内部任务（MLM，SOP等等）都没有过拟合
      2. dropout是为了降低过拟合而增加的机制，所以对于bert而言是弊大于利的机制

    * ![](https://i.imgur.com/nfWyxzD.jpg)

  * Results

    > An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. 
    >
    > We establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Specifically, we push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2. 

    ![](https://i.imgur.com/Tz13cl8.jpg)

    ![](https://i.imgur.com/7YwJ5P0.jpg)

    在相同的训练时间下，ALBERT得到的效果确实比BERT好

    在相同的Inference时间下，ALBERT base和large的效果都是没有BERT好的，而且差了2-3个点，作者在最后也提到了会继续寻找提高速度的方法（sparse attention和block attention）





