# 本周工作

1. 毕设

   设想的是：用BERT对Question和Column进行Encoder

   > 假设Question的长度为L，Question中每个Token为$T_{n,i}$；对应表中Column个数是$h_N$，$T_{h_i,M_i}$代表Token属于第$h_i$column的第$M_i$个字。我们采用SEP将Question和Column分割开来，所以输入到BERT的形式如下：
   >
   > $[CLS], T_{n,1}, T_{n,2},…,T_{n,L}, [SEP], T_{h_1,1}, T_{h_1,2},…,T_{h_1,M_1}[SEP],…, [SEP], T_{h_N,1}, …,T_{h_N,M_N}[SEP]$
   >
   > 其对应的segment embedding中，除了最后一个SEP为1，其余分隔符均为0，Question token为0，column为1，即：
   >
   > $0,0,0,…,0,0,1,1,…,1,0,…,0,1,…,1,1$

   但是实验发现：之前做wikiSQL时只用把Question+对应table放入BERT，但是由于spider数据集涉及到跨表查询，需要把整个schema（多个tables）一起放进BERT，导致out of memory（即便是batch size=1，也会OOV）。

   **思考**：如何兼顾GNN与BERT？

   【想法1】将生成SQL语句分成2步：

   * Step1 利用Question生成SQL sketch

     Question token represention 与 grammar-based decoding  

   * Step2 利用Question与Schema的linking填充细节

   ![](https://i.imgur.com/1TAZOtQ.jpg)

   【想法2】Multi-Hop
   
   ​	Cognitive Graph for Multi-Hop Reading Comprehension at Scale



2. 论文阅读

**目录**

|                           文章标题                           |  主题  |   来源   |
| :----------------------------------------------------------: | :----: | :------: |
| SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task | 语法树 | ACL 2018 |
|     X-SQL：reinforce schema representation with context      | MT-DNN | MSR 2019 |

# 1. SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task 

基于语法树结构生成SQL

![](https://i.imgur.com/4QdN9wh.jpg)

分为9个子模块：

![img](https://pic1.zhimg.com/80/v2-5f8172baef628df5392f7e351a80d7b8_hd.jpg)

基于树结构的循环生成：

![img](https://pic4.zhimg.com/80/v2-ffbcae42d21ae785136e399748db543b_hd.jpg)

从Root开始循环生成SQL语句，每步跟之前的方法差不多。

如果stack里进的是tuple，则这个tuple表示几个绑定一起的token。



# 2. X-SQL：reinforce schema representation with context

## ABSTRACT

X-SQL的目标是加强BERT类模型输出的contextual output在结构化的schema representation上的效果，可以为下流任务学习到新的schema representation。

## Introduction

之前的works用预训练模型的输出：要么是用于捕获序列信息，要么是直接用预训练模型的某一层输出去预测最终的output。它们并不能有效捕捉语义分析中的结构信息。

本文提出：an additional layer to reinforce the contextual information from pretraining models to the highly structural schema information.

 We treat this as the first attempt to reinforce the BERT-style contextual information into **a problem-dependent structure**, and then build **a new representation** to better characterize its structural information for downstream tasks.

wikisql的主要挑战是在于where clause的预测，之前的工作通常把这个问题视为multiple
binary classification problems with each for a column. 然而这个方法并不能有效的考虑columns之间的关系，因为它们的预测是独立的。

为了解决这个问题，X-SQL提出了新的objective: a list-wise global ranking approach using the KL divergence。并且未来解决SQL语句中有空where clause的情况，我们有意地在encoder中引入Empty column，并且把这个也视为global ranking prediction的一个候选。

## NEURAL ARCHITECTURE

![](http://ww3.sinaimg.cn/large/006tNc79ly1g4l8pec2fxj30yi0u078k.jpg)

包括3个部分encoder, context reinforcing layer and output layer.

1. Encoder

在用bert上，有以下不同：

* token [EMPTY]会被应用到每个table schema
* Segment embeddings for question and schema segment会被扩展为type embedding。我们会学习4种类型的的embedding：question，categorial column，numerical column and the special empty column。
* 用的不是Bert，MTDNN

* 用[CXT]替代[CLS]，来强调捕捉的context information，而不是下游任务的的表达

2. CONTEXT REINFORCING LAYER

![](http://ww4.sinaimg.cn/large/006tNc79ly1g4lcr529xmj30s601eq35.jpg)

上面是encoder的输出，每一个维度都是d。注意一下，每一个column name会包括多个token 。

我们的context reinforcing layer想要学习a new representation $r_{C_i}$ for each column，要实现这个可以通过：

strengthening the original encoder output **with the global context information captured in h[CXT].**

![](http://ww4.sinaimg.cn/large/006tNc79ly1g4ldjzkqdjj31700fedj6.jpg)

尽管（1）中已经是column多个token的加权和，还是会在schema representation 加上$h_[ctx]$，这会让column针对不同问题时representation不同。

context reinforcing layer 和 column attention 的目标都是获得NL question和table schema更好的对齐结构，它们的不同在于方法的不同和它们在entire architecture中的角色不同。

Column attention的目标是找到与each column最相关的query words。它对每一个column做这个策略，所以它与output layer联系紧密，而不是作为一个separate module。

 Context reinforcing laye认为BERT style encoder已经在NL这一边表现的很好了，它最关键的是能得到a better representation for schema。它只用[CTX] 获得的context information，后面就是接到schema 部分。

基于这些想法，OUTPUT LAYER中的每个子任务都会得到简化。

3. OUTPUT LAYER

output layer 的输入是：

![](http://ww4.sinaimg.cn/large/006tNc79ly1g4lf3f8w7dj317g0383zw.jpg)

本任务也将任务分成6个子模块。

select clause预测：

![](http://ww2.sinaimg.cn/large/006tNc79ly1g4lf6mcdnfj316k0emjvi.jpg)

where clause预测：

![](http://ww2.sinaimg.cn/large/006tNc79ly1g4lfhxe1wbj313g0ogwne.jpg)



