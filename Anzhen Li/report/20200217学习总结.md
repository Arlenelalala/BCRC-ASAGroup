## 本周学习总结

1. system1_Entity Linking

   * 再次对Base model进行分析，结论如下：

     ```
     1. 利用实体识别确实能对任务起到初步过滤的辅助作用
     2. table的预测与column的预测是相辅相成的
     3. 在字符匹配特征的基础上，再增加新特征去补充语义信息
     ```

   * 模型优化

     ```
     1. 对单表进行表征的过程加入Question
     2. 实体词保留token维度
     3. 增加其他特征作为辅助
     4. 预测层现在只做了对列的预测，再加上对表的预测作为辅助
     5. 更新loss function
     ```

2. 论文阅读

   AAAI2020 Improving Entity Linking by Modeling Latent Entity Type Information

   > **梗概：**
   > 作者通过分析Base model结果，发现之前模型没有利用到一个实体在句子中的Type Information，举例：
   >
   > ```
   > Fruits that tend to be more popular in this area are [MASK] , pears , and berries.
   > ```
   >
   > 我们就可以猜测这个[MASK]是一种水果。
   > 当然如果要在句子中一个一个标注这个Type是不切实际的，有的时候我们也只是有个模糊的感觉——这个Type大概是什么。所以作者非常巧妙地借鉴BERT训练任务——Mask任务，在包含实体词的句子中把实体词Mask掉，得到context representation：
   > 												$c_{ij} =BERT({lctx_{ij},[MASK],rctx_{ij}})$
   > 这个context representation就会包含有这个实体词在上下文的Type Information。
   > 然后把context representation加入到Base model中就是它的模型。
   >
   > 
   >
   > **点评：**
   > 这篇文章的Base model（Deep Joint Entity Disambiguation with Local Neural Attention EMNLP2017）是一篇创意十足的的文章。
   >
   > 而本文基于Base model，它的提高主要是来自于BERT encoder和基于BERT得到的context representation，作者能发现Type Information这个点以及能用到MASK的方式，还是小而巧的。
   >
   > 此外，本文的行文风格很不错，Introduction部分写得环环相扣，娓娓道来，实验部分做得非常充足。

   

## 下周工作安排

开始system2的优化

