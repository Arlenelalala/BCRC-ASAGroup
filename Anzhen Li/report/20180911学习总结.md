## 1. RNN（Recurrent Neural Networks）

### 1.1 Motivation

* 考虑序列关系，比如视频顺序、文本语序，可以自然而然地考虑将之前时间节点中的参数添加到下一个时间节点的运算中。</br>

</n>

### 1.2 RNN结构 正向推导

一个简单的RNN网络由输入层、隐藏层和输出层组成。</br>

X：input </br>

U：代表输入层到隐藏层的权重矩阵，即X的权重矩阵</br>

S：代表隐藏层的节点（可以是多个节点）</br>

V：代表隐藏层到输出层的权重矩阵</br>

O：output </br>

W：$S_{t-1}$的权重矩阵</br>

RNN的**隐藏层**的值$S_{t}$不仅仅取决于当前这次的输入x，还取决于上一次**隐藏层**的值$S_{t-1}$：
$$
S_{t} = f(U\cdot X_{t} + W \cdot S_{t-1})
$$
输出$O_{t}$：
$$
O_{t} = g(V \cdot S_{t})
$$
下图将RNN网络按照时间线展开，更清楚的表示了序列之间通过隐藏层建立了联系。

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fum1edym00j31jy0hcjtp.jpg)



![](https://ws2.sinaimg.cn/large/006tNbRwgy1fum1xm9jz7j31kw0nttfe.jpg)

</n>

### 1.3 RNN训练 

- **前向计算**
  $$
  net_{t}=U\cdot X_{t} + W \cdot S_{t-1} \\
  S_{t} = f(U\cdot X_{t} + W \cdot S_{t-1})
  $$
  即：
  $$
  \begin{bmatrix} s_{ 1 }^{ t } \\ s_{ 2 }^{ t } \\ … \\ s_{ n }^{ t } \end{bmatrix}=f(\begin{bmatrix} u_{ 11 } & u_{ 12 } & ... & u_{ 1m } \\ { u }_{ 21 } & { u }_{ 22 } & ... & { u }_{ 2m } \\ \quad  & \quad  & ... & \quad  \\ { u }_{ n1 } & { u }_{ n2 } & ... & { u }_{ nm } \end{bmatrix}\begin{bmatrix} x_{ 1 } \\ x_{ 2 } \\ … \\ x_{ m } \end{bmatrix}+\begin{bmatrix} w_{ 11 } & w_{ 12 } & ... & w_{ 1n } \\ { w }_{ 21 } & { w }_{ 22 } & ... & { w }_{ 2n } \\ \quad  & \quad  & ... & \quad  \\ { w }_{ n1 } & { w }_{ n2 } & ... & { w }_{ nn } \end{bmatrix}\begin{bmatrix} s_{ 1 }^{ t-1 } \\ s_{ 2 }^{ t-1 } \\ … \\ s_{ n }^{ t-1 } \end{bmatrix})
  $$
  其中：

  $w_{ji}$表示**循环层**第t-1时刻的第i个神经元到**循环层**第t个时刻的第j个神经元的权重。

  $u_{ji}$表示**输入层**第i个神经元到**循环层**第j个神经元的权重。

- **反向传播 **

  求$\partial E/ \partial W$，E是损失函数

  ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fuwox84hqnj306p03kwee.jpg)

  * 误差项

  ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuwp2gldcej30m307sq3i.jpg)

  BTPP算法将第l节点t时刻的**误差项**$\delta_{t}^{l} $值沿两个方向传播：

  - ==沿时间线传递到初始时刻，得到$\delta_{1}^{l} $，这部分只和权重矩阵W有关==

    我们用向量$net_{t}$表示神经元在t时刻的**加权输入**，因为：	
    $$
    net_{t} = U\cdot X_{t} + W \cdot S_{t-1}
    $$

    $$
    s_{t} = f(net_{t})
    $$

    因此：
    $$
    \frac { \partial net_{ t } }{ \partial net_{ t-1 } } =\frac { \partial net_{ t } }{ \partial s_{ t-1 } } \frac { \partial s_{ t-1 } }{ \partial net_{ t-1 } } \\ \quad \quad \quad \quad \quad 
    $$
    其中：

    ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuwn82ue09j30ms0kqgnb.jpg)

    ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuwn8n3cpjj30se0lktar.jpg)

    因此：
    $$
    \frac { \partial net_{ t } }{ \partial net_{ t-1 } } =\frac { \partial net_{ t } }{ \partial s_{ t-1 } } \frac { \partial s_{ t-1 } }{ \partial net_{ t-1 } } \\ \quad \quad \quad \quad \quad  =Wdiag[f^{ ' }(net_{ t-1 })]\\ \quad \quad \quad \quad \quad \quad\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  =\begin{bmatrix} { w }_{ 11 }f^{ ' }(net_{ 1 }^{ t-1 }) & { w }_{ 12 }f^{ ' }(net_{ 2 }^{ t-1 }) & ... & { w }_{ 1n }f^{ ' }(net_{ n }^{ t-1 }) \\ { w }_{ 21 }f^{ ' }(net_{ 1 }^{ t-1 }) & { w }_{ 22 }f^{ ' }(net_{ 2 }^{ t-1 }) & ... & { w }_{ 2n }f^{ ' }(net_{ n }^{ t-1 }) \\ \quad  & ... & \quad  & \quad  \\ { w }_{ n1 }f^{ ' }(net_{ 1 }^{ t-1 }) & { w }_{ n2 }f^{ ' }(net_{ 2 }^{ t-1 }) & ... & { w }_{ nn }f^{ ' }(net_{ n }^{ t-1 }) \end{bmatrix}
    $$
    因此将误差项沿时间反向传播的算法（E是误差函数）：

    ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuwnx0327oj30gv07cgm1.jpg)

  - ==传递到上一层网络，得到$\delta_{t}^{l-1} $，这部分只和权重矩阵U有关==

    ![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuwo2rfszdj30pd0f9jst.jpg)

  因此，权重矩阵W在t时刻的梯度$\triangledown _{ w_{ t } }E$:

  ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuwp3gg9acj30kw05qt92.jpg)

  最终的梯度$\triangledown _{ w }E$是各个时刻的梯度之和：

  ![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuwp4z1ik3j30o20723z2.jpg)

  同理可求$\partial E/ \partial U$

  ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fuwpfu4dc7j30m009zaaz.jpg)

  最后，用SGD更新权重。

</n>

### 1.5 缺陷 梯度爆炸与梯度消失

训练过程中，会出现$\partial E/ \partial W$非常陡峭或者非常平坦的情况，使得weight在更新中跳跃/不变化→梯度爆炸/梯度消失。

![](https://ws2.sinaimg.cn/large/0069RVTdly1fv5pog67qjj30yw0newiq.jpg)

* Why?

  ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fuwox84hqnj306p03kwee.jpg)

  ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuwnx0327oj30gv07cgm1.jpg)
  $$
  \frac { \partial net_{ t } }{ \partial net_{ t-1 } } =\frac { \partial net_{ t } }{ \partial s_{ t-1 } } \frac { \partial s_{ t-1 } }{ \partial net_{ t-1 } } \\ \quad \quad \quad \quad \quad  =Wdiag[f^{ ' }(net_{ t-1 })]\\ \quad \quad \quad \quad \quad \quad\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  =\begin{bmatrix} { w }_{ 11 }f^{ ' }(net_{ 1 }^{ t-1 }) & { w }_{ 12 }f^{ ' }(net_{ 2 }^{ t-1 }) & ... & { w }_{ 1n }f^{ ' }(net_{ n }^{ t-1 }) \\ { w }_{ 21 }f^{ ' }(net_{ 1 }^{ t-1 }) & { w }_{ 22 }f^{ ' }(net_{ 2 }^{ t-1 }) & ... & { w }_{ 2n }f^{ ' }(net_{ n }^{ t-1 }) \\ \quad  & ... & \quad  & \quad  \\ { w }_{ n1 }f^{ ' }(net_{ 1 }^{ t-1 }) & { w }_{ n2 }f^{ ' }(net_{ 2 }^{ t-1 }) & ... & { w }_{ nn }f^{ ' }(net_{ n }^{ t-1 }) \end{bmatrix}
  $$


![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuwpu7t9aej30gy04swem.jpg)

因为矩阵高次幂项的存在，使得$\beta_{w}\beta_{f}$的微小变化会造成非常大的影响。

* 梯度爆炸解决方法

  **clipping **
  clip_gradient 的引入是为了处理gradient explosion的问题。clip_gradient 的直观作用就是让权重的更新限制在一个合适的范围。</br>

  **Step ** </br>

  １．在solver中先设置一个clip_gradient </br>
  ２．在前向传播与反向传播之后，我们会得到每个权重的梯度diff，这时不像通常那样直接使用这些梯度进行权重更新，而是先求所有权重梯度的平方和sumsq_diff，如果sumsq_diff > clip_gradient，则求缩放因子scale_factor = clip_gradient / sumsq_diff。这个scale_factor在(0,1)之间。如果权重梯度的平方和sumsq_diff越大，那缩放因子将越小。 </br>
  ３．最后将所有的权重梯度乘以这个缩放因子，这时得到的梯度才是最后的梯度信息。这样就保证了在一次迭代更新中，所有权重的梯度的平方和在一个设定范围以内，这个范围就是clip_gradient. </br>

* 梯度消失解决方法

  * 从直观上来看

    在时序上的梯度消失会造成网络无法记住长久以前的信息，我们需要一个东西保存长久以前的信息。

## 2. LSTM Long Short-term Memory 

### 2.1 LSTM结构正向推导

### ![](https://ws2.sinaimg.cn/large/0069RVTdly1fv5un76qw3j313g0scag3.jpg)

Gate是什么？

![](https://ws4.sinaimg.cn/large/0069RVTdly1fv5unujsycj313a0tc798.jpg)
$$
g(z) = g(w_{z}x + b_{z})\\ 
f(z_{i})= \sigma(w_{i}x+b_{i}) \\
f(z_{f})= \sigma(w_{f}x+b_{f}) \\
f(z_{o})= \sigma(w_{o}x+b_{o}) \\
$$
![](https://ws1.sinaimg.cn/large/0069RVTdly1fv5uor4rgzj313i0u6wk6.jpg)

![](https://ws3.sinaimg.cn/large/0069RVTdgy1fv5x9mx8ffj31480smgos.jpg)
$$
c_{t} = f(z_f)\circ c_{t-1}+f(z_{i})\circ g(z)\\
y = h(c_{t}) \circ f(z_{o})
$$
实际中，我们还把$c_{t}$和$h_{t}$ 加入到input中去。

![](https://ws1.sinaimg.cn/large/0069RVTdgy1fv5ytcolwgj31480uqdlo.jpg)



### 2.2 LSTM训练  反向传播算法

* [长短时记忆网络的训练](https://zybuluo.com/hanbingtao/note/581764)

* lstm为何能解决梯度消失？

  ![](https://ws4.sinaimg.cn/large/0069RVTdly1fv6qlb6681j312o0lswk3.jpg)

</n>



