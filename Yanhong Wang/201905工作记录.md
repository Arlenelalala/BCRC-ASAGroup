[TOC]

## 快速跳转

[第一周20190502-20190508](#第一周)

[第二周20190509-20190516](#第二周)

[第三周20190517-20190523](#第三周)

[第四周20190517-20190523](#第四周)

## <span id="第一周">20190502-20190508</span>

### 周计划

* 完成统计学习的课程报告
* alphapose架构分析已经重新训练
* 模型压缩论文

### 周总结

* 论文阅读

  | 论文分类     | 论文名称                                                     | 阅读状态                  | 时间 |
  | ------------ | ------------------------------------------------------------ | ------------------------- | ---- |
  | 模型压缩综述 | A Survey of Model Compression and Acceleration for Deep Neural Networks，2017 CVPR | [简单笔记](#第一周论文一) |      |

* 资料阅读

  [模型压缩总览]( http://www.sohu.com/a/232047203_473283)

  [详细笔记](#详细笔记)

#### <span id="第一周论文一">**1. A Survey of Model Compression and Acceleration for Deep Neural Networks，2017 CVPR**</span>

* **Motivation**

  ![1556964969117](201905工作记录.assets/1556964969117.png)

* **Main idea**

* **Parameter pruning and sharing**

  * **Quantization and binarization**

    * [6-2014CVPR-Compressing Deep Convolutional Networks using Vector Quantization](https://arxiv.org/abs/1412.6115)

      * 这篇论文主要针对全连接层进行压缩，之前的方法都是采用矩阵分解方法，但是本篇采用了向量量化方法。在参数值上用kmeans方法，得到了8-16倍的压缩，0.5%的精度损失。进一步用结构量化方法，得到24倍压缩，1%精度损失。

      * 矩阵分解方法（数学真重要）
  
        我们使用[SVD](<https://blog.csdn.net/u011251945/article/details/81362642>)来分解参数矩阵，![1557130030086](201905工作记录.assets/1557130030086.png)，其中W是原始全连接层权重矩阵，大小是m*n，分解后可得到U为m\*k，S是k\*k的对角矩阵，V为n\*k，所以压缩比是mn/k(m+n+1)
  
      * **向量分解方法**（本文使用）
  
        * 二值化BINARIZATION
          ![1557133685229](201905工作记录.assets/1557133685229.png)
  
          这种方法可以对数据进行32倍的压缩，因为二值化。
  
        * 使用kMEANS的标量量化SCALAR QUANTIZATION USING kMEANS
  
          就是用kmeans方法做了个聚类，但是![1557134252864](201905工作记录.assets/1557134252864.png)这句话感觉不对吧，，这只能说是index的压缩，权重包含进去的话压缩率肯定没这么高啊，搞笑~
  
        * 乘法量化PRODUCT QUANTIZATION
          结构化向量量化
  
          其实就是把一个m*n的矩阵按照列分成s片，那么每一片都是m\*n/s，将每一片的一行看成kmeans聚类中的一个元素，也就是以一个1\*n/s长度的向量来聚类，最后理论压缩率是(32mn)/(32kn + log2(k)ms)
  
    * [7-2015CVPR- Quantized convolutional neural networks for mobile devices](https://arxiv.org/abs/1512.06473)
  
      6,7两篇文章都采用了k均值聚类算法来量化参数的值，k均值聚类算法可参考
  
    * k-means scalar;8-bit;16-bit;
  
      > 韩松提出的：普通学习——剪枝权重小的网络——用huffman编码剩余权重——重新训练refine一下现存连接
      >
      > hessian weight能表示网络参数的重要性：提出新的聚类方法，通过最小化hessian权重量化误差。
      >
      > 1-bit
    
      1-bit Drawback：
  
      ​	1.在处理大的CNN网络如GoogleNet时精确度会很差
  
      ​	2.现存的二值化框架基于简单的矩阵近似，忽略了二值化对精度的影响
  
      为了解决上述问题：
  
      [16]近似牛顿算法，对角海森近似，直接最小化二值化权重的损失
  
      [17]的工作通过随机二值化权值和将隐状态计算中的乘法转换为显式的变化，减少了训练阶段浮点数乘法的时间
  
  * **Pruning and Sharing**
  
    之前用来解决过拟合问题
  
    早期方法：
  
    > Biased Weight Decay
    >
    > Optimal Brain Damage [19] and the Optimal Brain Surgeon [20]:降低连接的数目，并且说pruning之后的精度可能会更好
  
    现在方法1：剪枝预训练CNN模型
  
    >以前的方法都是从头训练，现在流行一种**剪枝预训练模型的方法**
    >
    >韩松论文相当于之前方法一个总结：先移除多余连接，量化权重，最后权重聚类，用Huffman编码。
    >
    >基于soft weight-sharing的一种正则化方法
  
    现在方法2：训练紧凑的具有稀疏约束的CNN
  
    >稀疏约束是在正则化中被引入的，如l0正则化或者l1-norm正则化。
    >
    >[25]得到的群稀疏性约束，可以让我们进行group-wise fashion的操作
    >
    >[26]group-sparse regularizer
    >
    >[27]在每一层加上了一个结构化洗漱正则器。
  
    Drawback：
  
    l1,l2正则化比一般需要更多的迭代次数去收敛
  
    所有层需要手动设置敏感度，所以需要fine-tuning
  
  * **Designing Structural matrix**
  
    全连接层一直是瓶颈
    
    定义：x变为一个参数化结构矩阵，M变为比m*n表示稀疏得多的结构矩阵，不仅可以减少参数数量，还可以加速训练和推论的速度。
    
    循环预测：

****

#### <span id="详细笔记">模型压缩总览阅读</span>

[阅读资料](http://www.sohu.com/a/232047203_473283)

* 目前方案：

  低秩近似（low-rank Approximation），网络剪枝（network pruning），网络量化（network quantization），知识蒸馏（knowledge distillation）和紧凑网络设计（compact Network design）

* Low-rank：

  Previous low-rank based methods:

  [张量分解](<https://blog.csdn.net/flying_sfeng/article/details/87453217>)

  * SVD

    > *Zhang, Xiangyu, et al. "Accelerating very deep convolutional networks for classification and detection." *IEEE transactions on pattern analysis and machine intelligence* 38.10 (2016): 1943-1955.*

  

Tensor分解：

  * CP decomposition
  
    >Lebedev, Vadim, et al. "Speeding-up convolutional neural networks using fine-tuned cp-decomposition." 2014-CVPR . 
    >
    >[张量的CP分解](<https://zhuanlan.zhihu.com/p/25067269>)
    >
    >![è¿éåå¾çæè¿°](201905工作记录.assets/20180825185631219.png)
* Tucker decomposition
  
    >Kim, Yong-Deok, et al. "Compression of deep convolutional neural networks for fast and low power mobile applications." 2015-CVPR.
    >
  >[张量的Tucker分解](https://zhuanlan.zhihu.com/p/24798389)
    >
  >![è¿éåå¾çæè¿°](201905工作记录.assets/2018082710345255.png)
  
    * Tucker分解和CP分解的对比 
  
    CP分解是Tucker分解的特殊形式
  
    ![1557502089215](201905工作记录.assets/1557502089215.png)
  
  ![1557502107536](201905工作记录.assets/1557502107536.png)
  
  * Tensor Train Decomposition
  
    >Novikov, Alexander, et al. "Tensorizing neural networks." *Advances in neural information processing systems*. 2015.
    >
    >* [ref](<https://blog.csdn.net/flying_sfeng/article/details/87453217>)
    >
    >* Tensor-train Decomposition将原来的高维张量分解为多个三维张量的乘积（首尾张量为二维）
    >
    >  ![ å¨è¿éæå¥å¾çæè¿°](201905工作记录.assets/20190216153605531.png)
    >
    >![å¨è¿éæå¥å¾çæè¿°](201905工作记录.assets/20190216153236927.png)
  
  * Block Term Decomposition
  
    >Wang, Peisong, and Jian Cheng. "Accelerating convolutional neural networks for mobile applications." *Proceedings of the 24th ACM international conference on Multimedia*. ACM, 2016.
    >
    >* [ref]([http://www.xiongfuli.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017-06/tensor-decomposition-BTD.html](http://www.xiongfuli.com/机器学习/2017-06/tensor-decomposition-BTD.html))
    >
    >* ref中的一个思考很有意思，就是说CP分解中是讲一个稀疏张量分解为R个秩一张量相加，但是不同秩、或者说不同尺度的特征是不同的，这时候CP分解无法直观做到。将秩一张量换成秩k矩阵，那么就得到了BTD模型
    >
    >* ![1557567562462](201905工作记录.assets/1557567562462.png)
    >
    >  其中，![1557567647182](201905工作记录.assets/1557567647182.png)![1557567712850](201905工作记录.assets/1557567712850.png)
    >
    >  ![Block term decompositon(201905工作记录.assets/BTD-1557567783723.png)](http://www.xiongfuli.com/assets/img/201706/BTD.png)
    >
    >* 所以L，M，N怎么确定？
  
  为什么low-rank不再流行？
  
  * 对1*1卷积无效率
  * 在bottleneck结构中3*3卷积的计算复杂度并不大
  * 深度卷积，组卷积已经很快了
  
* Pruning

  ICLR2017斯坦福提出deep compression方法，但是随机剪枝对硬件非常不友好，所以就是用成块出现的pruning方法。

  * Structured pruning

    ****
    
    **对weights**
    
    >combined group and exclusive sparsity for deep nerual networks, 2017ICML
    >
    >* [补充数学知识-非凸正交正则化](<https://www.docin.com/p-1216479127.html>):有点太数学化了，看不下去
    >
    >* [补充数学知识-正则化]：看的链接找不到了，但是分享资料满天飞，随便看
    >
    >* [补充数学知识-理解凸优化](<https://zhuanlan.zhihu.com/p/37108430>)：讲的不错
    >
    >* 论文阅读：
    >
    >论文提出了对权重进行分析剪枝我的方法，首先用Group Sparsity组稀疏的方法对分组特征添加稀疏正则来修剪掉权重矩阵的一些列，然后通过Exclusive Sparsity增强不同权重之间特征的竞争力来学习更有效的filters，两者共同作用取得了很好的Pruning结果。
    >
    >![1557631809015](201905工作记录.assets/1557631809015.png)
    
    ****
    
    **对feature map和activation**
    
    >Learning Efficient Convolutional Networks through Network Slimming，2017ICCV
      >
    >![1557632262820](201905工作记录.assets/1557632262820.png)
      >
    >论文思想很简单，给每个通道channel添加一个尺度因子scaling factor，然后对这些尺度因子scaling factor添加sparsity regularization，最后根据尺度因子大小对相应的通道channels进行修剪，将一些尺度因子比较小的通道剪掉，实现对整个网络的瘦身效果。
    
    ****
    
    **对gradients**
    
    >
    >
    >
    
      
    
  * Filter Pruning 
  
  * Gradient Pruning
  
  * Fine-grained Pruning in a Bayesian View
  
    

 

## <span id="第二周">20190509-201905015</span>

### 周计划

- 完成统计学习的课程报告
- alphapose架构分析已经重新训练
- 模型压缩论文

### 周总结

* 统计学习的课程报告完成，细看略看了一些论文，压缩的主要框架，常用方法了解得差不多了，下周可以给大家作报告

- alphapose代码研究

  - sppe重新训练

    sppe重新训练分为两个步骤：用coco2017数据训练，加上PNG（对数据进行随机生成）训练，现在只实现了第一个部分，计划实现第二部分训练的同时对第一部分进行压缩，测试准确度等。

  * Pytorch学习时的一些启发
    * 步态数据训练时候对于分类数据进行one-hot编码
    * 步态数据对于正常数据进行标准化 
    * 关于风格的loss定义和内容的loss定义

## <span id="第三周">20190516-20190522</span>

### 周计划

* sail-sppe之数据准备（[简要思路记录](#sail-sppe思路)）
* 准备闵老师课程报告和组会报告（有需要的话）
* 读论文thundernet（陈佳禹推荐，不好就揍他）

### 周总结

* sail-sppe数据准备工作

  

- 论文阅读

  | 论文分类     | 论文名称                                | 阅读状态                  | 时间 |
  | ------------ | --------------------------------------- | ------------------------- | ---- |
  | 目标检测论文 | CenterNet :Objects as Points，2019 CVPR | [简单笔记](#第一周论文一) |      |

- 资料阅读

  [模型压缩总览]( http://www.sohu.com/a/232047203_473283)

  [详细笔记](

#### <span id="sail-sppe思路">1.sail-sppe思路</span>

* Motivation

  现有的up-bottom方法的关键点检测都是两步，一是检测人物的bounding box，二是对bounding box中的人物进行关键点检测。大多使用的数据集是coco这种。

  > coco: 
  >
  > * 91类目标，328,000影像和2,500,000个label
  >
  > - 250,000 people with keypoints

  其中图片场景类别非常多样化，得到的网路的鲁棒性也很好，但随之而来的是网络参数量和计算量剧增，运行效率很低。在sail应用场景中，我们应用场景非常明确，我们完全有理由相信，通过折损一定的泛化性，我们可以得到一个在速度和精度上都更高的网络。

* Experiments

  * 数据准备

    我们延续两步走策略，需要的数据集是**图片+bounding box+节点坐标**

    * 图片的获取

      将目前所得的所有步态视频按帧读取，得到数据集。

      需要注意的是：

      1.逐帧读取的话就要注意数据过拟合问题，所以训练集和测试集不能来自相同的视频

      2.数据读取的时候注意深度视频和彩色视频的帧匹配问题。 

    * Way1：

      bounding box和节点坐标通过将网络输入到alphapose中获取。

  * 实验

  * 可预见的困难

* Results

#### 2. 论文

##### 2.1 CenterNet :Objects as Points，2019 CVPR

[参考博客](<https://blog.csdn.net/c20081052/article/details/89358658>)

![1558496316365](201905工作记录.assets/1558496316365.png)

**1. Main Idea:**

  本文通过目标中心点来呈现目标（见图2），然后在中心点位置回归出目标的一些属性，例如：size, dimension, 3D extent, orientation, pose。 而**目标检测问题变成了一个标准的关键点估计问题**。我们仅仅将图像传入全卷积网络，得到一个热力图，**热力图峰值点即中心点，每个特征图的峰值点位置预测了目标的宽高信息**。

**2.Related Work**

2.1 Object detection with implicit anchors

* Faster RCNN

* anchor shape priors [44, 45], different feature resolution [36], and loss re-weighting among different samples [33].

* 其实我们的中心点就可以看做是传统目标检测中的锚点获取，但是：

  1.我们只基于位置，不基于框，所以不需要前景后景的分类;

  2.我们对于每个类别只有一个正锚点，不需要NMS，我们只从keypoint map中产生局部峰值；

  3.用大分辨率图像

2.2 Object detection by keypoint estimation

* CornerNet:预测左上角和右下角
* ExtremeNet：预测四个角和中心
* 但是他们都需要聚类，很耗时。我们的Centernet因为只有一个中心点输出，所以不需要聚类或者其他后处理。

2.3 Monocular 3D object detection

* Deep3Dbox：slow-RCNN [19] style framework：先检测2D物体，喂入3D网络
* 3D RCNN:在Faster-RCNN基础上加了一个头
* Deep Manta：corsetofine Faster-RCNN
* 我们工作和前两个的一阶段形式很像，但是更简单更快

**3. Preliminary**

* input：![1558580745334](/home/wyh/.config/Typora/typora-user-images/1558580745334.png)

* output：![1558580772661](/home/wyh/.config/Typora/typora-user-images/1558580772661.png)![1558580785622](/home/wyh/.config/Typora/typora-user-images/1558580785622.png)注意在这里C是值对弈每一个类别关键点的个数，所以对于每一个分类都有这么一个map，最终输出还要乘上分类。

* 损失函数：

  * focal loss

    ![1558580827063](/home/wyh/.config/Typora/typora-user-images/1558580827063.png)

  * offset loss

    ![1558580845305](/home/wyh/.config/Typora/typora-user-images/1558580845305.png)

* 这个只是输出每一个类的一个feature map，并且每个类中只要一个目标，**所以有一个很大的弊端就是对于同一个类有多个目标没办法检测**。

* 还有一个问题就是如果某个类在某张训练图片中不存在的话那损失怎么计算。

**4.Objects as Points**

![1558583678799](/home/wyh/.config/Typora/typora-user-images/1558583678799.png)

![1558583709519](/home/wyh/.config/Typora/typora-user-images/1558583709519.png)

​	points to bounding boxes:

![1558583748128](/home/wyh/.config/Typora/typora-user-images/1558583748128.png)

所以输出的是C+4

**4.1. 3D detection**

需要增加的是points depth(1)，3D dimension(3) 和orientation(本来是1，但是因为比较难回归，所以用8表示)

所以输出的是C+4+12

**4.2. Human pose estimation**

其实没太看懂，各种channel什么的。

## <span id="第四周">20190523-20190529</span>

### 周计划

- centernet复现
- keypoint做检测方向论文
- 步态数据集建立

### 周总结

- 论文阅读

  | 论文分类                    | 论文名称                                                     | 阅读状态                  | 时间 |
  | --------------------------- | ------------------------------------------------------------ | ------------------------- | ---- |
  | 目标检测轻量级,anchor free  | CenterNet :Objects as Points，2019 CVPR                      | [简单笔记](#第一周论文一) | 5.24 |
  | 目标检测轻量级,anchor free  | Cornernet：Detecting objects as paired keypoints             |                           | 5.25 |
  | 目标检测轻量级，anchor free | Bottom-up Object Detection by Grouping Extreme and Center Points<br/>Xingyi | [简单笔记](#extremenet)   |      |
  |                             |                                                              |                           |      |

- 代码实践

  [centernet代码实践](#centernet代码)

#### <span id="centernet代码">1.centernet代码实践</span>

#####  1.1. 代码install：

* undefined symbol: __cudaRegisterFatBinaryEnd 

  原因：http://www.cnblogs.com/luruiyuan/p/10720581.html

  解决方案：卸载原来的pytorch，安装cuda10版本对应的

* 好烦啊，pytorch换成了1.1.0版本，但是作者用到了`from torch.utils.ffi import _wrap_function`，但是在pytorch1.1.0ffi已经弃用了，于是改成`from torch.utils.cpp_extension import _warp_function`，然而吧，在cpp_extension中有没有_warp_function这个函数，对pytorch的函数和神经网络具体过程一无所知的我并不知道该用什么函数代替

* 最终在金创服务器159上的实现了，但是inference过程顺利，train过程总是出现cuda的out of memory问题。



* 非常想实现这个代码，感觉这个新奇的思想会是以后的一个热门方向，但是，，，，我自闭了，，等到明天再试吧，，，

#### 2.Alphapose代码实践

alphspose官方放出三版代码：#

| 版次   | 框架    | 备注                                                       | 实现情况                                                     |
| ------ | ------- | ---------------------------------------------------------- | ------------------------------------------------------------ |
| 第一版 | lua     | 大概是因为第一版伴随着论文开源的缘故，写得很认真很详细     | 未实现                                                       |
| 第二版 | pytorch | 最方便安装，但是好像只有sppe的训练代码，没有整体的训练代码 | 实现了sppe，但是也没有作者的单独的sppe结果做对比，不知道复现效果怎么样，就当练手吧 |
| 第三版 | mxnet   | 一个新的框架，代码简介，但是框架集成了很多东西             | 没能成功复现，这个要不要继续尝试再说吧                       |



a.原版用lua

b.第二版用pytorch

#### <span id="extremenet">3.Extremenet论文阅读</span>

**Abstract**

说是和two stage达到了一样的效果，43.7% on coco test-dev，但是没提到速度。说是得到了一个粗略的mask。

**1.Introduction**

矩形框不一定是物体最自然的表达

物体不一定是轴对称的，矩形框会损失背景信息，自上而下的策略计算了很多无用框，效率很低。

**检测出来四个extreme坐标，聚类时候按照n^4进行匹配，只有匹配的中心在center map上的得分高于预设阈值才算一个正例。**





































