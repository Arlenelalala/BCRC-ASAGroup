[TOC]





## 20190502-20190508

### 周计划

* 完成统计学习的课程报告
* alphapose架构分析已经重新训练
* 模型压缩论文

### 周总结

* 论文阅读

  | 论文分类     | 论文名称                                                     | 阅读状态                  | 时间 |
  | ------------ | ------------------------------------------------------------ | ------------------------- | ---- |
  | 模型压缩综述 | A Survey of Model Compression and Acceleration for Deep Neural Networks，2017 CVPR | [简单笔记](#第一周论文一) |      |

* 资料阅读

  [模型压缩总览]( http://www.sohu.com/a/232047203_473283)

  [详细笔记](#详细笔记)

#### <span id="第一周论文一">**1. A Survey of Model Compression and Acceleration for Deep Neural Networks，2017 CVPR**</span>

* **Motivation**

  ![1556964969117](201905工作记录.assets/1556964969117.png)

* **Main idea**

* **Parameter pruning and sharing**

  * **Quantization and binarization**

    * [6-2014CVPR-Compressing Deep Convolutional Networks using Vector Quantization](https://arxiv.org/abs/1412.6115)

      * 这篇论文主要针对全连接层进行压缩，之前的方法都是采用矩阵分解方法，但是本篇采用了向量量化方法。在参数值上用kmeans方法，得到了8-16倍的压缩，0.5%的精度损失。进一步用结构量化方法，得到24倍压缩，1%精度损失。

      * 矩阵分解方法（数学真重要）
  
        我们使用[SVD](<https://blog.csdn.net/u011251945/article/details/81362642>)来分解参数矩阵，![1557130030086](201905工作记录.assets/1557130030086.png)，其中W是原始全连接层权重矩阵，大小是m*n，分解后可得到U为m\*k，S是k\*k的对角矩阵，V为n\*k，所以压缩比是mn/k(m+n+1)
  
      * **向量分解方法**（本文使用）
  
        * 二值化BINARIZATION
          ![1557133685229](201905工作记录.assets/1557133685229.png)
  
          这种方法可以对数据进行32倍的压缩，因为二值化。
  
        * 使用kMEANS的标量量化SCALAR QUANTIZATION USING kMEANS
  
          就是用kmeans方法做了个聚类，但是![1557134252864](201905工作记录.assets/1557134252864.png)这句话感觉不对吧，，这只能说是index的压缩，权重包含进去的话压缩率肯定没这么高啊，搞笑~
  
        * 乘法量化PRODUCT QUANTIZATION
          结构化向量量化
  
          其实就是把一个m*n的矩阵按照列分成s片，那么每一片都是m\*n/s，将每一片的一行看成kmeans聚类中的一个元素，也就是以一个1\*n/s长度的向量来聚类，最后理论压缩率是(32mn)/(32kn + log2(k)ms)
  
    * [7-2015CVPR- Quantized convolutional neural networks for mobile devices](https://arxiv.org/abs/1512.06473)
  
      6,7两篇文章都采用了k均值聚类算法来量化参数的值，k均值聚类算法可参考
  
    
  
    > k-means scalar;8-bit;16-bit;
    >
    > 韩松提出的：普通学习——剪枝权重小的网络——用huffman编码剩余权重——重新训练refine一下现存连接
    >
    > hessian weight能表示网络参数的重要性：提出新的聚类方法，通过最小化hessian权重量化误差。
    >
    > 1-bit
  
    1-bit Drawback：
  
    ​	1.在处理大的CNN网络如GoogleNet时精确度会很差
  
    ​	2.现存的二值化框架基于简单的矩阵近似，忽略了二值化对精度的影响
  
    为了解决上述问题：
  
    [16]近似牛顿算法，对角海森近似，直接最小化二值化权重的损失
  
    [17]的工作通过随机二值化权值和将隐状态计算中的乘法转换为显式的变化，减少了训练阶段浮点数乘法的时间
  
  * **Pruning and Sharing**
  
    之前用来解决过拟合问题
  
    早期方法：
  
    > Biased Weight Decay
    >
    > Optimal Brain Damage [19] and the Optimal Brain Surgeon [20]:降低连接的数目，并且说pruning之后的精度可能会更好
  
    现在方法1：剪枝预训练CNN模型
  
    >以前的方法都是从头训练，现在流行一种**剪枝预训练模型的方法**
    >
    >韩松论文相当于之前方法一个总结：先移除多余连接，量化权重，最后权重聚类，用Huffman编码。
    >
    >基于soft weight-sharing的一种正则化方法
  
    现在方法2：训练紧凑的具有稀疏约束的CNN
  
    >稀疏约束是在正则化中被引入的，如l0正则化或者l1-norm正则化。
    >
    >[25]得到的群稀疏性约束，可以让我们进行group-wise fashion的操作
    >
    >[26]group-sparse regularizer
    >
    >[27]在每一层加上了一个结构化洗漱正则器。
  
    Drawback：
  
    l1,l2正则化比一般需要更多的迭代次数去收敛
  
    所有层需要手动设置敏感度，所以需要fine-tuning
  
  * **Designing Structural matrix**
  
    全连接层一直是瓶颈
    
    定义：x变为一个参数化结构矩阵，M变为比m*n表示稀疏得多的结构矩阵，不仅可以减少参数数量，还可以加速训练和推论的速度。
    
    循环预测：

****

#### <span id="详细笔记">模型压缩总览阅读</span>

* 目前方案：

  低秩近似（low-rank Approximation），网络剪枝（network pruning），网络量化（network quantization），知识蒸馏（knowledge distillation）和紧凑网络设计（compact Network design）

* Low-rank：

  Previous low-rank based methods:

  * SVD

    > *Zhang, Xiangyu, et al. "Accelerating very deep convolutional networks for classification and detection." *IEEE transactions on pattern analysis and machine intelligence* 38.10 (2016): 1943-1955.*

    

  * CP decomposition

    >Lebedev, Vadim, et al. "Speeding-up convolutional neural networks using fine-tuned cp-decomposition." 2014-CVPR .
    >
    >[参考理解](https://blog.csdn.net/yixianfeng41/article/details/73009210)
    >
    >[张量](https://wenku.baidu.com/view/1f72a551580102020740be1e650e52ea5418ce74.html)
    >
    >

  * Tucker decomposition

    >Kim, Yong-Deok, et al. "Compression of deep convolutional neural networks for fast and low power mobile applications." 2015-CVPR.
    >
    >

  * Tensor Train Decomposition

    >Novikov, Alexander, et al. "Tensorizing neural networks." *Advances in neural information processing systems*. 2015.
    >
    >

  * Block Term Decomposition

    >Wang, Peisong, and Jian Cheng. "Accelerating convolutional neural networks for mobile applications." *Proceedings of the 24th ACM international conference on Multimedia*. ACM, 2016.
    >
    >



