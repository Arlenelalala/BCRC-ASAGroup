[TOC]

## 快速跳转

[第一周20190530-20190605](#第一周)

[第二周20190606-20190612](#第二周)



## <span id="第一周">20190530-20190605</span>

### 周计划

- sppe用sail-color进行训练：完成，结果挺好
- sail-color数据集评估：完成，看了一遍
- sppe:coco训练结果评估：coco训练结果呢
- sppe:sail-color训练结果评估：
- 比较sppe的coco训练集结果和sail-color训练集结果：在sail数据集上精度差别不大，coco上差别比较大
- 论文阅读

### 周总结

- **论文阅读**

  | 论文分类                       | 论文名称 | 阅读状态 | 时间 |
  | ------------------------------ | -------- | -------- | ---- |
  | 这周没有读论文，哭唧唧，我错了 |          |          |      |


* **用sail-color训练sppe**

  coco数据量144213，其中val部分是5887

  所以在sail404中train30077，其中可以用1000来做val

  训练中的一些想法：

  * ~~因为数据集中是一个一个视频切割的，所以在划分train datasets和valid datasets的时候，应该随机划分，但是为了保证每次一样，只能进行一次随机，所以应该对数据做一个预处理。~~

    >仔细思考了一下这个问题，我们本来就要求在一些人的视频上进行训练，而能达到在另一些视频上
    >
    >比较好的效果，所以按照视频而不是图片为单位划分train和val是合理的，如果以图片为单位，很容易造成过拟合，但是因为相邻的视频也存在来源相近的问题，所以可以考虑在视频层次上进行shuffle。

  * coco数据集中有很多是没有人的，但是我们数据集中所有都是有人的，这个有没有影响？

  * sail404数据集中其实人一般只有两个状态，坐着，站着，或者可以先做个分类再分别做识别？

  * sppe训练好之后其实也要先detect然后再进行keypoints detect，所以找一个比较好的detection也是很重要的在exp1中shuffle设置成为了False，所以在训练中是按照输入的数据集的顺序进行训练的，所以在valid accu测试中，一个epoch中数据很容易突然分别集中分布在坐下，站起的状态中，或者是只有某一个人或者某两个人的图片，训练也是，这样是非常不利的，所以在exp2的时候一定要注意避免这个问题。

  ****

  **Exp1**：

  第一次训练：epoch=50  batch_size=128  total_num=30077  valid_num=1000

  结果如下图，分析：

  * 在valid上非常不稳定

  ![1559465896165](201906工作记录.assets/1559465896165.png)

* **sail-color数据集评估**

  * 因为标签就是用alphapose运行结果得到的，所以没有精标准，采取人工加速检查标注好skeleton的视频进行复检，得到有问题的视频有39，

* **coco数据集评估**

  * 作者使用的coco数据集每张可能有很多人，也可能没有人，有很多人的框选是任选一个框

  * 17个关键点在coco中如果被遮挡的话则认为是0：

    统计了一下在144213个图片中，各个关键点**被遮挡的个数**为：

    | 1      | 2      | 3      | 4      | 5      | 6         | 7         | 8      | 9         |
    | ------ | ------ | ------ | ------ | ------ | --------- | --------- | ------ | --------- |
    | Nose   | LEye   | REye   | LEar   | REar   | LShoulder | RShoulder | LElbow | RElbow    |
    | 43951  | 56978  | 56479  | 68426  | 67496  | 16897     | 16658     | 42541  | 40907     |
    | **10** | **11** | **12** | **13** | **14** | **15**    | **16**    | **17** | **total** |
    | LWrist | RWrist | LHip   | RHip   | LKnee  | Rknee     | LAnkle    | RAnkle |           |
    | 51080  | 48722  | 34170  | 33847  | 61235  | 61072     | 74862     | 74841  | 144213    |
    
    所以在作者使用的这个数据集中，被遮挡的现象还是比较严重的，尤其是脚踝部分（**我们最注重的地方**）被遮挡率甚至达到了50%。
    
    | 1                                                       | 2                                                       |
    | ------------------------------------------------------- | ------------------------------------------------------- |
    | ![000000089093](201906工作记录.assets/000000089093.jpg) | ![000000390731](201906工作记录.assets/000000390731.jpg) |
  
  * 有些标注把背景中的图片上的人物或者是镜子中反射的人物也标注了出来，这也是实际应用中容易出错的一个地方

* **coco训练sppe结果评估**

  coco训练出来的sppe在coco数据集上的准确度为57.04%

  在sail-vilid上的准确度为94.85%

* **sail数据集训练sppe结果评估**

  sail训练出来的sppe在sail-valid训练集上的准确度为97%

  在coco-valid上的上的准确度为21.12%

* ![1560911106113](201906工作记录.assets/1560911106113.png)

<<<<<<< HEAD
## <span id="第二周">20190606-20190612</span>

### 周计划

- efficientnet坐等更新训练的pytorch代码
- 尝试换更小网络进行sppe
- 视频多目标跟踪了解（源于https://blog.csdn.net/u011707542/article/details/79151978）[详细](#多目标跟踪)
- 复习课程
- 看论文

### 周总结

#### 1.多目标跟踪

计算机视觉四大任务：https://blog.csdn.net/u011707542/article/details/79151978

看了上面这个博客，突然觉得其实关键点检测这个问题从跟踪的角度来思考完全可以用下面两种方法代替：

* 单目标跟踪（人，代替检测）+sppe
* 多目标跟踪（关键点跟踪）

#### 2.alphapose运行

yolo+sppe(resnet101):对一个415 frames 的视频：

![1559874116080](201906工作记录.assets/1559874116080.png)

发现一个特别好用的东西：https://blog.csdn.net/xiemanr/article/details/69398057

******
### pytorchAlphapose架构

#### 1.SPPE

![graph_run=](201906工作记录.assets/graph_run=.png)
>>>>>>> bcfa3717d2f2148408a17634b1514daa4bdc7025

![1560152516232](201906工作记录.assets/1560152516232.png)





https://blog.csdn.net/u012426298/article/details/81093667 