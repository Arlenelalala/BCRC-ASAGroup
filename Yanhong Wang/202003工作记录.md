[TOC]

## 快速跳转：

[20200302](#3.1)

[20200309](#3.2)

[20200316](#3.3)

## <span id="3.2">20200309</span>

### 1. Papers

#### 1.1 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

* **Info:**
  Google AI Language
  
* **Related domain：**

  * [【NLP】Attention原理和源码解析：](https://zhuanlan.zhihu.com/p/43493999)

  * [【NLP】Transformer详解](https://zhuanlan.zhihu.com/p/44121378)
    * [Sequence to Sequence学习简述](https://www.jianshu.com/p/9bf7178279dc)

  * [【NLP】语言模型和迁移学习](https://zhuanlan.zhihu.com/p/42618178)
  * [【NLP】Google BERT详解](https://zhuanlan.zhihu.com/p/46652512)
  * [李宏毅老师的一些课程](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_2.html)
  * [All the ways you can compress BERT](http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html?nsukey=eHJM%2BGUiYs6qXlZDlmFREcOz2MH5BWOVeeChDwj1Yk1EyUzBERPxA4JMdFCJM%2Bifjl8UHbEGbTF6M3X2fMeG9CYeRRN55Xbezm3Z%2BDx077StqhxJkFh3e5HF7%2Feju5vfpGF99EIIVa%2BovT5V7hIUWMuQDfkXa177Dds0F5h8G0odZvGebtfVE27t9xYSYOkEAFLW4bleyUC3QtjCuFBM9w%3D%3D)

* **Proposed methods：**

  还没看

* **My view**：

#### 1.2 An Analysis of Neural Language Modeling at Multiple Scales

- **Info:**
  未发表，缺创新，偏实验

- **Related domain：**

  - [QRNN](https://blog.csdn.net/u011961856/article/details/77431869)
  - [BPTT](https://blog.csdn.net/u014038273/article/details/83042185)

- **Proposed methods：**

  就是分别用LSTM和QRNN在三个数据集：character-level (Penn Tree- bank, enwik8) and word-level (WikiText-103) datasets上做实验，得到了比较好的结果。

  发现在word level上QRNN表现更好，character level上LSTM表现更好，但是QRNN也可以通过加深layers来达到差不多的performance。

  网络架构基本上是Encoder-LSTM(或者QRNN)-Decoder。其中Encoder和Decoder之间使用了tied weights

  本文还对网络中的一大堆参数做了评测，针对不同的数据集找出了几个最优的参数值，weight dropout, hidden dropout and embedding dropout等

- **My view**：

  就比较工程，看它是为了理解代码

#### 1.3 Automatic Timed Up-and-Go Sub-Task Segmentation for Parkinson’s Disease Patients Using Video-Based Activity Classification

* **Info:**

### 2. Works

![1583724347624](202003工作记录.assets/1583724347624.png)

## 20200316

### 1. Works

#### 1.1 CC: Column combining

Aim: To reduce the number of parameters, improve memory storage efficiency and reduce bandwidth requirements.

![1584343860574](202003工作记录.assets/1584343860574.png)

>1. U={0,1,2,3,4,5,6,7}   G=[{}]
>2. U={}   G=[{0,2,4},{1,7},{3,5,6}]

#### 1.2 CPCC: Conflict-permitted Column Combining

![1583970701016](202003工作记录.assets/1583970701016.png)

![1583970712124](202003工作记录.assets/1583970712124.png)

![1583970723686](202003工作记录.assets/1583970723686.png)

* **Joint comparison of num_weights and test BPC between CC and CPCC**

![1583976106451](202003工作记录.assets/1583976106451.png)

#### 1.3 FM_CC

![1584345732578](202003工作记录.assets/1584345732578.png)

![1584345758510](202003工作记录.assets/1584345758510.png)

![1584345767338](202003工作记录.assets/1584345767338.png)

## 20200323

### 1. Work

#### 1.1. **Block column combine — Row split** 

![1584948348666](202003工作记录.assets/1584948348666.png)

![1584948398072](202003工作记录.assets/1584948398072.png)

<font color=red> Another interesting heatmap:</font>

 ![1584948459755](202003工作记录.assets/1584948459755.png)

#### 1.2. Quantitation with different section

* Observation:

![1584948511765](202003工作记录.assets/1584948511765.png)

* Experiments：

  ![1584948588679](202003工作记录.assets/1584948588679.png)

![1584948631301](202003工作记录.assets/1584948631301.png)

#### 1.3 CC_FM with CNN on CIFAR10

![1584948681882](202003工作记录.assets/1584948681882.png)

![1584948692576](202003工作记录.assets/1584948692576.png)

### 2. Paper and Lecture

CVPR2020: https://github.com/extreme-assistant/CVPR2020-Paper-Code-Interpretation/blob/master/CVPR2020.md#15

#### 2.1 Visual Commonsense R-CNN

很哲学的一篇文章：使用干预的手法去寻找因果关系

![1584949369983](202003工作记录.assets/1584949369983.png)

#### 2.2 AutoML-NAS：

[华为高级研究员谢凌曦: 下一代人工智能计算模型探索](https://mp.weixin.qq.com/s/1dGZTQYtlbhtncm5yeLLcA)

其实模型压缩除了

* 参数上：裁剪，量化
* 模型上：学生网络，人工直接紧身设计精巧的网络之外

NAS从模型的构建上来说，也算是一种从模型上自动压缩的方法。

**EXAMPLE:DARTS**

* A cell-based search space (Repeatable cells, the number of cells is adjustable)
* A large operation set (Within each cell, there is a directed acyclic graph)

![1584949990543](202003工作记录.assets/1584949990543.png)

![1584950162559](202003工作记录.assets/1584950162559.png)

#### 2.3 Quantization：

[模型量化——更小更快更强](https://b23.tv/BV19J411R7t2) ：MIT的一个博士介绍的，主要讲了模型量化的总览以及分享了他们的一些量化方向的工作。

#### 2.4 GNN:

[从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型](https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html)

### **3. Plan**

* Column combine方法：集成FM和CP想法实验[原论文](https://arxiv.org/abs/1811.04770)上的tasks，与原论文结果对比。