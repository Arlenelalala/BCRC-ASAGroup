使用snownlp包对鹿晗微博评论进行分析
=================
## 代码
代码构成：
1. 爬虫部分
2. 训练部分
3. 分析部分
4. 生成词云部分

### 爬虫部分
分别爬取鹿晗和关晓彤微博下面的评论，为了加快时间，爬取100页约1000条评论；  
代码如下：
```python
#!/usr/bin/env python
# -*- coding:utf-8 -*-
_author_ = 'Gavin'

# -*- coding:utf-8 -*-

import requests
import re
import time
import pandas as pd

# 把id替换成你想爬的地址id
urls = 'https://m.weibo.cn/api/comments/show?id=4160547165300149&page={}'

headers = {'Cookies': 'SUB=_2A252TonYDeRhGeNO7VUQ9ybPyjyIHXVVsBeQrDV6PUJbkdBeLWWmkW1NTvajAmqJghhQzfULU7d2dWvhwUT1cIHj; SUHB=0C1CNwOPUjl8OX; SCF=AtaeAdQXp5Pp15Rhi0bnbQMv_8-2wpBQ2_920tTVi1TsEbIshQIyddd1XOO6yDF1MifcVNRFt3AqhzbjEIOvTlY.; _T_WM=fa5cb6262f00578fe322471470ee127c; WEIBOCN_FROM=1110006030; MLOGIN=1; M_WEIBOCN_PARAMS=oid%3D4160547694498927%26luicode%3D20000061%26lfid%3D4160547694498927%26uicode%3D20000061%26fid%3D4160547694498927',
           'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'}

# 找到html标签
tags = re.compile('</?\w+[^>]*>')


# 设置提取评论function
def get_comment(url):
    j = requests.get(url, headers=headers).json()
    comment_data = j['data']['data']
    for data in comment_data:
        try:
            comment = tags.sub('', data['text'])  # 去掉html标签
            reply = tags.sub('', data['reply_text'])
            weibo_id = data['id']
            reply_id = data['reply_id']

            comments.append(comment)
            comments.append(reply)
            ids.append(weibo_id)
            ids.append(reply_id)

        except KeyError:
            print('KeyError!!!!!!')


comments, ids = [], []
for i in range(1,101):
    url = urls.format(str(i))
    get_comment(url)
    time.sleep(1)  # 防止爬得太快被封

df = pd.DataFrame({'ID': ids, '评论': comments})
df = df.drop_duplicates()
df.to_txt('鹿晗.txt', index=False, encoding='utf-8')
```
爬取关晓彤微博评论类似，不再重复。

### 训练部分
由于snownlp自带的语料库是对商品的评价，使用微博评论语料库对其重新训练，可以使效果更好；  
从网上下载微博500w语料库（未标注）,对snownlp训练，会生成一个新的neg.txt和pos.txt和sentiment.marshal.3文件，使用新生成的sentiment.marshal.3文件替换掉原来的文件，重新跑分析部分；<br>
训练部分代码如下：
```python
#!/usr/bin/env python 
# -*- coding:utf-8 -*-
_author_ = 'Gavin'

import re
from snownlp import sentiment
import numpy as np
import pymysql
from snownlp import SnowNLP
import matplotlib.pyplot as plt
from snownlp import sentiment
from snownlp.sentiment import Sentiment
conn = pymysql.connect(host='localhost', user='root', password='', charset="utf8",use_unicode=False)  # 连接服务器
with conn:
    cur = conn.cursor()
    cur.execute("SELECT * FROM test.token1 WHERE weiboId < '%d'" % 6000000) #token1里面有10w条数据
    rows = cur.fetchall()
comment = []
for row in rows:
    row = list(row)
    comment.append(row[18])
def train_model(texts):
    for li in texts:
        comm = li.decode('utf-8')
        text = re.sub(r'(?:回复)?(?://)?@[\w\u2E80-\u9FFF]+:?|\[\w+\]', ',',comm)
        socre = SnowNLP(text)
        if socre.sentiments > 0.8:
            with open('pos.txt', mode='a', encoding='utf-8') as g:
                g.writelines(comm +"\n")
        elif socre.sentiments < 0.3:
            with open('neg.txt', mode='a', encoding='utf-8') as f:
                f.writelines(comm + "\n")
        else:
            pass

train_model(comment)
sentiment.train('neg.txt', 'pos.txt')
sentiment.save('sentiment.marshal')
```
这里用到一点数据库MySQL的知识，将下载好的微博语料库导入MySQL数据库，使用python调用它，但是在训练时，500w内容太多，导致电脑死机，对数据库进行分割，产生100w的子集，使用100w的语料库训练，产生结果。
### 分析部分
使用训练过的snownlp分析微博评论，使用snownlp的sentiments模块，将微博的评论情感画在一个直方图上面；<br>
代码如下：
```python
#!/usr/bin/env python 
# -*- coding:GBK -*-
_author_ = 'Gavin'

import codecs
import re
import numpy as np
import pymysql
from snownlp import SnowNLP
import matplotlib.pyplot as plt
from snownlp import sentiment
from snownlp.sentiment import Sentiment

comment = []
with open('F:\python_project\luhan500w\关晓彤.txt', mode='r', encoding='utf-16') as f:
    rows = f.readlines()
    for row in rows:
        if row not in comment:
            comment.append(row.strip('\n'))
def snowanalysis(self):
    sentimentslist = []
    for li in self:
        text = re.sub(r'(?:回复)?(?://)?@[\w\u2E80-\u9FFF]+:?|\[\w+\]', ',',li)
        print(li)
        s = SnowNLP(li)
        print(s.sentiments)
        sentimentslist.append(s.sentiments)
    plt.hist(sentimentslist, bins=np.arange(0, 1, 0.01))
    plt.show()
snowanalysis(comment)
```
### 生成词云部分
将两个人微博的评论进行分词和词频统计，生成词云，可以更直观的表现两个人微博评论的异同  
代码如下：
```python
#!/usr/bin/env python 
# -*- coding:utf-8 -*-
_author_ = 'Gavin'

import pickle
from os import path
import jieba
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
def make_worldcloud(file_path):
    text_from_file_with_apath = open(file_path,'r',encoding='UTF-16').read()
    wordlist_after_jieba = jieba.cut(text_from_file_with_apath, cut_all=False)
    wl_space_split = " ".join(wordlist_after_jieba)
    print(wl_space_split)
    backgroud_Image = plt.imread('F:\python_project\luhan500w\ciyun2.jpg')
    print('加载图片成功！')
    '''设置词云样式'''
    stopwords = STOPWORDS.copy()
    stopwords.add("哈哈") #可以加多个屏蔽词
    stopwords.add("回复")
    wc = WordCloud(
        width=1024,
        height=768,
        background_color='white',# 设置背景颜色
        mask=backgroud_Image,# 设置背景图片
        font_path='F:\python_project\luhan500w\simsun.ttf',  # 设置中文字体，若是有中文的话，这句代码必须添加，不然会出现方框，不出现汉字
        max_words=600, # 设置最大显示的字数
        stopwords=stopwords,# 设置停用词
        max_font_size=400,# 设置字体最大值
        random_state=50,# 设置有多少种随机生成状态，即有多少种配色方案
    )
    wc.generate_from_text(wl_space_split)#开始加载文本
    img_colors = ImageColorGenerator(backgroud_Image)#产生背景图片
    wc.recolor(color_func=img_colors)#字体颜色为背景图片的颜色
    plt.imshow(wc)# 显示词云图
    plt.axis('off')# 是否显示x轴、y轴下标
    plt.show()#显示
    # 获得模块所在的路径的
    d = path.dirname(__file__)
    # os.path.join()：  将多个路径组合后返回
    wc.to_file(path.join(d, "关晓彤评论词云.jpg"))
    print('生成词云成功!')

make_worldcloud('F:\python_project\luhan500w\关晓彤.txt')
```

