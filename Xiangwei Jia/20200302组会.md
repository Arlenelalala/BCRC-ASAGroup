# Deep Interest Network for Click-Through Rate Prediction

> Guorui Zhou, Chengru Song, Xiaoqiang Zhu Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai
>
> Alibaba Group
>
> **KDD 2018**
>
> https://arxiv.org/pdf/1706.06978.pdf

传统的CTR深度学习模型范式：`Embedding+MLP`

+ 嵌入层（用于学习稀疏特征的密集表示）
+ MLP（用于自动学习特征的组合关系）

流程：

+ 将大规模稀疏输入特征映射到低维嵌入向量
+ 然后以分组方式转换为固定长度的矢量
+ 最后将这些特征串联在一起，传入多层感知器 (MLP) 中，进而学习特征之间的非线性关系。

弊端：

+ 用户特征都被压缩成固定长度的表示向量。

  > 使用固定长度的向量是一个瓶颈，这给`Embedding&MLP`方法从丰富的**历史行为中有效地捕捉用户的多种兴趣**带来困难。
  >
  > 通常通过求和/平均池将对应的嵌入向量列表转换为固定长度的向量，这会导致信息丢失。

+ Diversity：用户在浏览电商网站的兴趣多样性。

+ Local activation: 由于用户兴趣的多样性，只有部分历史数据会影响到当次推荐的物品是否被点击，而不是所有的历史记录

本文提出了一种新的模型：深度兴趣网络(DIN)，通过一个**局部激活单元**（local activation unit）来自适应地从用户的历史行为中学习该用户**对某一广告的兴趣**，并得到一种表示向量，进而解决了上述难题。***该表示向量因广告不同而不同***，这就大大提高了模型的表达能力。



为了训练具有数亿个参数的工业界深度网络，还提出了两种技巧：

+ 小批量感知正则化（mini-batch aware regularization） 
+ 数据自适应激活函数，（data adaptive activation function）

文章的创新点：

+ 指出了使用固定长度向量来表达用户的各种兴趣的局限性，并设计了一个新颖的深度兴趣网络（DIN），该网络引入了局部激活单元，。给定广告，可以从历史行为中自适应学习用户兴趣的表示。 DIN可以极大地提高模型的表达能力并更好地捕捉用户兴趣的多样性特征。
+ 我们开发了两种新颖的技术来帮助训练工业深度网络：i）一种微型批量感知正则器，它可以节省大量参数对深度网络的正则化计算 ii）数据自适应激活函数，通过考虑输入的分布来概括PReLU，并显示出良好的性能。
+ 我们对公共数据集和Al-ibaba数据集进行了广泛的实验。结果验证了提出的DIN和培训技术的有效性。



DIN的主要特性在于**局部激活单元**以及**weighted sum pooling**

+ 局部激活单元来软搜索相关的用户行为
+ 采用加权和池以获取针对给定广告的用户兴趣的自适应表示

==> **用户表示向量在不同广告上有所不同**



Deep Interest Network（DIN）

+ 通过考虑给定候选广告的历史行为的相关性，自适应地计算用户兴趣的表示向量。
+ 通过引入局部激活单元，DIN通过软搜索历史行为的相关部分来关注相关的用户兴趣，
+ 并采用加权sum池化来获取有关候选广告的用户兴趣的表示形式。 

> 与候选广告相关性更高的行为会获得更高的激活权重，并且支配着用户兴趣。
>
>  这样，**用户兴趣的表示向量随广告的不同而变化**，从而提高了模型在有限尺寸下的表达能力，并使DIN能够更好地捕获用户的不同兴趣。

![](https://cdn.mathpix.com/snip/images/y55ah8aXw8kAAXw37fbmxQV0wAunIIxEPU2uPAjMgSU.original.fullsize.png)

> 嵌入和池化层均以逐组方式运行，将原始稀疏特征映射到多个固定长度表示形式的向量。
>
> 然后将所有向量连接在一起以获得实例的整体表示向量



常规操作：

Embedding layer --> Pooling layer and Concat layer --> MLP --> Loss
$$
L=-\frac{1}{N} \sum_{(x, y) \in \mathcal{S}}(y \log p(x)+(1-y) \log (1-p(x)))
$$
![](https://cdn.mathpix.com/snip/images/5wr5Ehgzj2ISCBRsqm2O0nM6WeIAdMMEkCZDvo1QwGE.original.fullsize.png)

图2：网络架构。左侧部分说明了基本模型（Embedding＆MLP）的网络。 cate_id，shop_id和goods_id的嵌入属于一种商品，它们的组合表示用户行为中的一种交互商品。右侧部分是我们建议的DIN模型。它引入了一个**局部激活单元**，用户兴趣的表示可根据给定的不同候选广告而自适应地变化。

### Attention:

传统的Attention机制中，给定两个item embedding，比如u和v，通常是直接做点积uv或者uWv，其中W是一个|u|x|v|的权重矩阵，但这篇paper中阿里显然做了更进一步的改进，着重看上图右上角的activation unit，**首先是把u和v以及u v的element wise差值向量合并起来作为输入，然后喂给全连接层，最后得出权重，**这样的方法显然损失的信息更少。



可以发现：

+ Base Model对用户的历史行为是同等对待的，没有做任何处理，这显然是不合理的。一个很显然的例子，离现在越近的行为，越能反映你当前的兴趣。因此，DIN模型对用户历史行为基于Attention机制进行一个加权；
+ 

$$
\boldsymbol{v}_{U}(A)=f\left(\boldsymbol{v}_{A}, \boldsymbol{e}_{1}, \boldsymbol{e}_{2}, \ldots, \boldsymbol{e}_{H}\right)=\sum_{j=1}^{H} a\left(\boldsymbol{e}_{j}, \boldsymbol{v}_{A}\right) e_{j}=\sum_{j=1}^{H} \boldsymbol{w}_{j} \boldsymbol{e}_{j}
$$

> + {e1，e2，...，eH}是长度为H的用户U行为的嵌入向量列表，
> + vA是广告A的嵌入向量。
> + vU（A）随不同广告而变化。
> + a（·）是一个前馈网络，其输出作为激活权重，如图2所示。
> + 除了两个输入嵌入向量之外，a（·）将它们的乘积相加以馈入后续网络，这是帮助进行关联建模的显式知识

Attention机制简单的理解就是，针对不同的广告，用户历史行为与该广告的权重是不同的。这里的权重，就是Attention机制即上图中的Activation Unit所需要学习的。

如果不用Local activation的话，将会出现下面的情况：假设用户的兴趣的Embedding是Vu，候选广告的Embedding是Va，用户兴趣和候选的广告的相关性可以写作F(U,A) = Va * Vu。如果没有Local activation机制的话，那么同一个用户对于不同的广告，Vu都是相同的.

在加入Activation Unit之后，用户的兴趣表示计算如下：



![img](https:////upload-images.jianshu.io/upload_images/4155986-482bdce2c1bee47c.png?imageMogr2/auto-orient/strip|imageView2/2/w/501/format/webp)



其中，Vi表示behavior id i的嵌入向量，比如good_id,shop_id等。Vu是所有behavior ids的加权和，表示的是用户兴趣；Va是候选广告的嵌入向量；wi是候选广告影响着每个behavior id的权重，也就是Local Activation。wi通过Activation Unit计算得出，这一块用函数去拟合，表示为g(Vi,Va)。



模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式：



![img](https://upload-images.jianshu.io/upload_images/4155986-358cc3c6656a5408.png?imageMogr2/auto-orient/strip|imageView2/2/w/654/format/webp)

我们首先要肯定的是，AUC是要分用户看的，我们的模型的预测结果，只要能够保证对每个用户来说，他想要的结果排在前面就好了。

那么GAUC的计算，不仅将每个用户的AUC分开计算，同时根据用户的展示数或者点击数来对每个用户的AUC进行加权处理。进一步消除了用户偏差对模型的影响。通过实验证明，GAUC确实是一个更加合理的评价指标。

**Dice激活函数**

尽管对Relu进行了修正得到了PRelu，但是仍然有一个问题，即我们认为分割点都是0，但实际上，分割点应该由数据决定，因此文中提出了Dice激活函数

Dice激活函数的全称是Data Dependent Activation Function，形式如下：



![img](https:////upload-images.jianshu.io/upload_images/4155986-2ff44bcf7d3b4054.png?imageMogr2/auto-orient/strip|imageView2/2/w/375/format/webp)



其中，期望和方差的计算如下：



![img](https:////upload-images.jianshu.io/upload_images/4155986-448758600f0d336f.png?imageMogr2/auto-orient/strip|imageView2/2/w/606/format/webp)



可也看到，每一个yi对应了一个概率值pi。pi的计算主要分为两步：将yi进行标准化和进行sigmoid变换。

![](https://cdn.mathpix.com/snip/images/U_SNnHZw-5d68TrgHi5wf56Prq3QauKafWkiLeo2Phg.original.fullsize.png)



###  自适应正则 Adaptive Regularization

CTR中输入稀疏而且维度高，通常的做法是加入L1、L2、Dropout等防止过拟合。但是论文中尝试后效果都不是很好。用户数据符合长尾定律，也就是说很多的feature id只出现了几次，而一小部分feature id出现很多次。这在训练过程中增加了很多噪声，并且加重了过拟合。

对于这个问题一个简单的处理办法就是：直接去掉出现次数比较少的feature id。但是这样就人为的丢掉了一些信息，导致模型更加容易过拟合，同时阈值的设定作为一个新的超参数，也是需要大量的实验来选择的。

因此，阿里提出了**自适应正则**的做法，即：
 1.针对feature id出现的频率，来自适应的调整他们正则化的强度；
 2.对于出现频率高的，给与较小的正则化强度；
 3.对于出现频率低的，给予较大的正则化强度。

计算公式如下：



![img](https:////upload-images.jianshu.io/upload_images/4155986-69a8d1e143b1953a.png?imageMogr2/auto-orient/strip|imageView2/2/w/583/format/webp)

### 结果

下图是对使用不同正则项的结果进行的展示，可以发现，使用自适应正则的情况下，模型的验证集误差和验证集GAUC均是最好的。



![img](https:////upload-images.jianshu.io/upload_images/4155986-aff9654a61c327ce.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)



下图对比了Base Model和DIN的实验结果，可以看到，DIN模型在加入Dice激活函数以及自适应正则之后，模型的效果有了一定的提升：![img](https:////upload-images.jianshu.io/upload_images/4155986-d4ed014c118ab98b.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

+ 模型贡献

+ 用GAUC这个离线metric替代AUC

+ 用Dice方法替代经典的PReLU激活函数

+ 介绍一种Adaptive的正则化方法

# DIEN

> Deep Interest Evolution Network for Click-Through Rate Prediction
>
> AAAI 2019



在用DIN解决了用户的兴趣不同的问题后，模型还存在以下问题

1. 用户的兴趣是不断进化的，而DIN抽取的用户兴趣之间是独立无关联的，没有捕获到兴趣的动态进化性
2. 通过用户的显式的行为来表达用户隐含的兴趣，缺乏针对具体行为背后的潜在兴趣的专门建模，这一准确性无法得到保证。

===> 所以DIEN仍然采用了attention机制，捕获与目标商品相关商品的兴趣发展进化的路径。

为了解决以上两个问题，阿里算法又提出了DIEN模型

+ 设计**兴趣提取层**( interest extractor layer)以从历史行为序列中捕获临时兴趣。在这一层，我们引入了辅助损失，以监督每一步的利息提取。
+ 由于用户兴趣的多样性，尤其是在电子商务系统中，我们提出了兴趣演化层( interest evolving layer)以捕获相对于目标商品的兴趣发展过程。在兴趣演化层，注意力机制被嵌入到序列结构中，并且在兴趣演化过程中增强了相对兴趣的影响。

在针对公共和工业数据集的实验中，DIEN的性能明显优于最新解决方案。值得注意的是，DIEN已被部署到淘宝的展示广告系统中，其点击率提高了20.7％。

总结：DIEN最大的特点是不但要找到用户的interest，还要抓住用户interest的进化过程。作者们将GRU融合到网络中，从而抓出变化的sequence。

1. DIEN关注电商场景中兴趣演化的过程，并提出了新的网络结构来建模兴趣进化的过程，这个模型能够更精确的表达用户兴趣，同时带来更高的CTR预估准确率。
2. 设计了兴趣抽取层，并通过计算一个辅助loss，来提升兴趣表达的准确性。
3. 设计了兴趣进化层，来更加准确的表达用户兴趣的动态变化性。

![](https://cdn.mathpix.com/snip/images/Am6B0dtfJ3J_sLGlZnd7O8pD1XLuizn_P4nLP6zFAJ4.original.fullsize.png)

可以看到，新模型较之于base model和DIN，其他部分保持一致，变化只在于用户行为序列（UBS）的处理做了调整，将其组织成了序列数据的形式。

模型包含了3个部分：

+ 最底层是Behavior Layer，用于将用户浏览过的商品转换成对应的embedding，并且按照浏览时间做排序，
+ 中间层是兴趣提取层Interest Extractor Layer，
+ 最上层是兴趣发展层Interest Evolving Layer，

### 兴趣抽取层Interest Extractor Layer

如上图中黄色区域所示，兴趣抽取层Interest Extractor Layer的主要目标是从embedding数据中提取出interest。但一个用户在某一时间的interest不仅与当前的behavior有关，也与之前的behavior相关，所以作者们使用GRU单元来提取interest，GRU输出h(t)如下所示：
$$
\begin{array}{l}
\mathbf{u}_{t}=\sigma\left(W^{u} \mathbf{i}_{t}+U^{u} \mathbf{h}_{t-1}+\mathbf{b}^{u}\right) \\
\mathbf{r}_{t}=\sigma\left(W^{r} \mathbf{i}_{t}+U^{r} \mathbf{h}_{t-1}+\mathbf{b}^{r}\right) \\
\tilde{\mathbf{h}}_{t}=\tanh \left(W^{h} \mathbf{i}_{t}+\mathbf{r}_{t} \circ U^{h} \mathbf{h}_{t-1}+\mathbf{b}^{h}\right) \\
\mathbf{h}_{t}=\left(\mathbf{1}-\mathbf{u}_{t}\right) \circ \mathbf{h}_{t-1}+\mathbf{u}_{t} \circ \tilde{\mathbf{h}}_{t}
\end{array}
$$
h(t)是提取出的用户的初步兴趣表示，为了更好地表达用户的兴趣，作者还为兴趣抽取层引入了一个有监督学习：

> 输入是按照时间步排列的商品embedding向量，假设第t个时间步输入e(t)，GRU输出隐单元h(t)，令下一个时间步的输入向量e(t+1)作为正样本，随机采样负样本![[公式]](https://www.zhihu.com/equation?tex=e%28t%2B1%29%27)，且![[公式]](https://www.zhihu.com/equation?tex=e%28t%2B1%29%27%21%3De%28t%29)，h(t)与正负样本向量分别做内积，得到预测结果，并引入一个辅助的logloss：

$$
\begin{aligned}
L_{a u x}=-& \frac{1}{N}\left(\sum_{i=1}^{N} \sum_{t} \log \sigma\left(\mathbf{h}_{t}^{i}, \mathbf{e}_{b}^{i}[t+1]\right)\right.\\
&\left.+\log \left(1-\sigma\left(\mathbf{h}_{t}^{i}, \hat{\mathbf{e}}_{b}^{i}[t+1]\right)\right)\right)
\end{aligned}
$$

$$
L=L_{\text {target}}+\alpha * L_{\text {aux}}
$$

![[公式]](https://www.zhihu.com/equation?tex=L_%7Btarget%7D)是CTR任务的logloss函数，将CTR的loss和辅助loss相加定义为整个网络的loss进行优化。![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)是超参数来平衡两个loss的权重。

这里辅助loss好处：当GRU处理较长序列的时候有助于降低反向传播的难度，对于商品embedding的学习也有助益。

### 兴趣进化层Interest Evolution Layer

有了用户的兴趣表示，Evolution Layer的主要目标就是去刻画用户兴趣的演变过程（观测到兴趣随环境和认知的变化而变化）。因此作者设计了AUGRU结构，用attention机制来局部激活与目标商品相关的局部兴趣，并对存在于这些兴趣之间的依赖建模，捕捉兴趣的演变过程。

具体如模型中红色区域所示，**这里使用了第2个GRU。将目标商品的embedding向量与第1个GRU的输出隐向量发生交互，生成attention score**，计算公式如下所示：
$$
a_{t}=\frac{\exp \left(\mathbf{h}_{t} W \mathbf{e}_{a}\right)}{\sum_{j=1}^{T} \exp \left(\mathbf{h}_{j} W \mathbf{e}_{a}\right)}
$$
其中，![[公式]](https://www.zhihu.com/equation?tex=h_t)是隐向量，![[公式]](https://www.zhihu.com/equation?tex=e_a)是目标商品的嵌入向量，W是需要训练的参数矩阵。最后用softmax对attention score进行归一化。

![](https://cdn.mathpix.com/snip/images/CWWETeLgmLscqqclqmuG2wr1HNmlzDywrKHiwrG0FjM.original.fullsize.png)

> 先从AIGRU开始，从上面的公式中可以看出，AIGRU 激活局部兴趣和捕获兴趣演变的过程是相互独立的。仅仅是用attention来影响GRU的输入，且即便输入为 0（无关的兴趣）也还是会对hidden state产生影响；
>
> AGRU用 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bt%7D) 替代 GRU 的 update gate，直接控制 hidden state 的更新，将 attention 机制融入到了捕获兴趣演变的过程中，一定程度上弥补了 AIGRU 的不足。
>
> 但是，原先 GRU 中控制 hidden state 更新的是一个包含多个维度的向量，AGRU 用纯量替代略有不妥，因此这篇文章设计了 AUGRU，用 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bt%7D) 影响 ![[公式]](https://www.zhihu.com/equation?tex=u_%7Bt%7D) ，以间接作用于 hidden state的更新。  

![](https://cdn.mathpix.com/snip/images/RyFqcoi2BcpdZs8fIQAhn-n-mcTYW-NfqxE20jvDlds.original.fullsize.png)

# DSIN

> Deep Session Interest Network for Click-Through Rate Prediction
>
> Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu and Keping Yang
>
> Alibaba Group,  Zhejiang University
>
> IJCAI2019
>
> https://www.ijcai.org/Proceedings/2019/0319.pdf

# BST

> Behavior Sequence Transformer for E-commerce Recommendation in Alibaba
>
> ACM2019

DIN、DIEN、DSIN和BST模型的区别和联系：

1. DIN模型使用注意力机制来捕获目标商品与用户先前行为序列中商品之间的相似性，但是未考虑用户行为序列背后的序列性质，并且未捕捉用户兴趣的动态变化性。

2. DIEN主要解决DIN无法捕捉用户兴趣的动态变化性的缺点，提出了兴趣抽取层Interest Extractor Layer、兴趣进化层Interest Evolution Layer。

3. DSIN针对DIN和DIEN没考虑用户历史行为中的会话信息，因为在每个会话中的行为是相近的，而在不同会话之间差别是很大的。DSIN主要是在session层面上来利用用户的历史行为序列信息。

4. BST模型通过Transformer模型来捕捉用户历史序列中各个item的关联特征，并且通过加入待推荐的商品item，也可抽取出行为序列中商品与待推荐商品之间的相关性。