**论文阅读**

Empower Sequence Labeling with Task-Aware Neural Language Model

An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis

DOER: Dual Cross-Shared RNN for Aspect Term-Polarity Co-Extraction

**实验进展**

Exploiting BERT for End-to-End Aspect-based Sentiment Analysis 

论文github库　https://github.com/lixin4ever/BERT-E2E-ABSA

allennlp https://github.com/allenai/allennlp

transformers https://github.com/huggingface/transformers

跑了论文代码，结果可以达到论文的结果。论文的代码使用了allennlp transformers两个库，看他们的代码，学习怎么使用BERT，没怎么看懂，后面再看看。



根据BERT的论文以及github库的说明，进行fine-tuning。对于序列标注任务，论文的实现方式是模型最后一层的隐藏状态输入进一个分类层。

目前将分类层换成BiLSTM+CRF.目前已经完成数据处理以及代码。

后续考虑加入char embedding跟self-attention.