# 论文阅读随笔

1、Xception中的 tower

2、CNN中的pooling

### 20191015

##### Xception中的 tower

 tower 指的是 Inception 模块内的各个 path 

### 20191020

 2个3x3的卷积核替代5x5的卷积核，3个3x3的卷积核替代7x7的卷积核，5个3x3的卷积核替代11x11的卷积核可以减少计算量，且最终的计算结果是一样的，以图片是32*32的为例（stride=1）：

一个5*5卷积： (32-5)/1+1=28 ；28x28

两个卷积核为3 x3：第一层3x3：得到的结果是(32-3)/1+1=30；第二层3x3：得到的结果是(30-3)/1+1=28

### 20191022

##### CNN中的pooling

 Poolig层对Filter层的特征进行降维操作，形成最终的特征。一般在Pooling层后连接全连接层神经网络，形成最后的分类结果。 

###### Max  pooling

含义： 是对某个Filter抽取到若干特征值，只取得其中最大的那个Pooling层作为保留值，其他特征值全部抛弃，值最大代表只保留这些特征中最强的，抛弃其他弱的此类特征。 forward的时候你只需要把2x2窗子里面那个最大的拿走就好了，backward的时候你要把当前的值放到之前那个最大的位置，其他的三个位置都弄成0。 

优点：

-  保证特征的位置与旋转不变性。对于图像处理这种特性是很好的，但是对于NLP来说特征出现的位置是很重要的。比如主语一般出现在句子头等。
- 减少模型参数数量，减少过拟合问题。2D或1D的数组转化为单一数值，对于后续的convolution层或者全连接隐层来说，减少了单个Filter参数或隐层神经元个数。
- 可以把变长的输入x整理成固定长度的输入。CNN往往最后连接全连接层，神经元个数需要固定好，但是cnn输入x长度不确定，通过pooling操作，每个filter固定取一个值。有多少个Filter，Pooling就有多少个神经元，这样就可以把全连接层神经元固定住

缺点：

- 特征的位置信息在这一步骤完全丢失。在卷积层其实是保留了特征的位置信息的，但是通过取唯一的最大值，现在在Pooling层只知道这个最大值是多少，但是其出现位置信息并没有保留；
- 另外一个明显的缺点是：有时候有些强特征会出现多次，比如我们常见的TF.IDF公式，TF就是指某个特征出现的次数，出现次数越多说明这个特征越强，但是因为Max Pooling只保留一个最大值，所以即使某个特征出现多次，现在也只能看到一次，就是说同一特征的强度信息丢失了。

###### mean-pooling 

含义： 即对邻域内特征点只求平均：假设pooling的窗大小是2x2, 在forward的时候，就是在前面卷积完的输出上依次不重合的取2x2的窗平均，得到一个值就是当前mean pooling之后的值。backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了。 

特征提取的误差主要来自两个方面：（1）邻域大小受限造成的估计值方差增大；（2）卷积层参数误差造成估计均值的偏移。一般来说，mean-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。 

###### Stochastic-pooling 

含义： 介于两者之间，通过对像素点按照数值大小赋予概率，再按照概率进行亚采样，在平均意义上，与mean-pooling近似，在局部意义上，则服从max-pooling的准则。 