# C model

1.首先验证底层乘加操作与Verilog结果完全一致。

2.继续整体C model的编写，目前基本完成了PE阵列的C model，可以实现3×1的卷积，经验证，结果正确。

# 编译器

先看了《深入理解计算机系统》的第七章“链接”。

读了《LLVM编译器实战教程》第三章“工具与设计”，这一章主要是介绍了LLVM的核心理念和基本设计原理。

以上两本书我是结合起来看的，效果挺好的，《深入理解计算机系统》里面的东西大而全，而《LLVM》更加细节。

IR

XLA

MLIR

Halide，Halide是用C++作为宿主语言的一个图像处理相关的DSL(Domain Specified Language)语言，全称领域专用语言。主要的作用为在软硬层面上(与算法本身的设计无关)实现对算法的底层加速。

TVM的输入为：tensor、一系列lambda表达式和相应的schedule，然后解析器生成中间表达，中间表达经过一系列编译优化，最后通过代码生成器产生相应的源代码或机器代码。事实上，对于NVIDIA
GPU而言，有两种方式输出代码，一种是直接生成CUDA源码，另一种是通过LLVM生成PTX code，再经过cuda runtime 
driver编译成cubin代码。

常见深度学习编译器，包括Google XLA, MLIR，Facebook Glow/TC, DLMC TVM, MIT Halide/Tiramisu, Intel Stripe, Nvidia Diesel。深度学习编译器，一般是分两阶段优化，一个是high level optimization, 譬如在XLA里叫HLO， 这部分做硬件无关优化； 还有一个阶段是代码生成阶段，即codegen，和硬件体系结构相关。不同的编译器在这俩阶段的处理上各有所长。第一阶段的问题基本上被解决了，难的是第二阶段。

# 论文阅读

读了几篇DAC19的加速器相关的文章，还有一些算法方面的文章。

1. Chen, F., Song, L., Li, H. H., & Chen, Y. (n.d.). ZARA: A novel zero-free dataflow accelerator for generative adversarial networks in 3D ReRAM. *Proceedings - Design Automation Conference*. https://doi.org/10.1145/3316781.3317936

   出自DAC19，这一篇是一个GAN的加速器，它针对transposed convolution的resouce underutilization做了改进。transposed convolution会引入大量的0，而作者提出了一种新的mapper和scheduler避免了这些0造成的资源浪费。

2. 2.Ryu, S., Kim, H., Yi, W., & Kim, J. J. (2019). BitBlade: Area and energy-efficient precision-scalable neural network accelerator with bitwise summation. *Proceedings - Design Automation Conference*, 1–6. https://doi.org/10.1145/3316781.3317784

   也出自DAC19，是提出了一种precision-scalabel的神经网络加速器，我的理解他们的加速器可以通过配置来得到不同的cost与precision的tradeoff。不过具体实现还没有仔细研究

3. **3.**Brasó, G., & Leal-Taixé, L. (2019). Learning a Neural Solver for Multiple Object Tracking. Retrieved from http://arxiv.org/abs/1912.07515

   这一篇是上个月出现的一篇MOT论文，使用了图网络的方式来处理MOT问题，是基于Message Passing Networks（MPNs）做的。是offline方法，不过效果非常好，目前在MOT17排名第一，看格式应该投稿了CVPR2020，作者说论文接收后会放出代码，值得研究研究。

4. Anonymous. (2020). How much Position Information Do Convolutional Neural Networks Encode? *Iclr*, 1–10. Retrieved from https://openreview.net/forum?id=rJeB36NKvB

   这一篇被ICLR接收为Spotlight论文，主要研究了CNN能否编码绝对位置信息，挺有意思的，值得一看。

5. Simonyan, K., Dieleman, S., Senior, A., & Graves, A. (2016). WaveNet. *ArXiv Preprint ArXiv:1609.03499v2*, 1–15. https://doi.org/10.1109/ICASSP.2009.4960364

   因为在做那个加速器项目，好像要部署WaveNet，所以也研究了一下WaveNet的结构。