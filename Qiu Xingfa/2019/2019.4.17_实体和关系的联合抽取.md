# 2019.4.17_实体和关系的联合抽取

---
## [Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme](https://arxiv.org/pdf/1706.05075.pdf)

### Abstract
* 实体和关系的联合抽取在信息抽取领域是一个重要的任务，把联合抽取任务转换成一个标注问题，实验结果表示，用标注方法比现有的pipelined 和 joint learning 方法都要好。

### Intraduction
* 实体和关系的联合抽取是在非结构化的实体中同时识别实体和它们之间的关系，与open information extraction中关系词是在文本中抽取的方不同的是，这项任务中，关系词是预先定义好的，不一定会在文本中出现。
* 传统方法解决这个问题，首先是提取实体，然后识别它们之间的关系，但这就会忽略掉这两项子任务之间的关联，实体识别的结果会影响关系分类的结果，
* 与pipeline方法不同的是，联合学习框架是使用同一个模型来提取实体和关系，现有的一些联合方法都是基于特征的系统，它们需要复杂的特征设计以及非常依赖于其他的NLP工具包（这也会造成错误传播），为了减少手工的工作...发表了一篇用神经网络的端到端的实体和关系抽取的方法，虽然这个联合模型可以通过参数共享来同时同时代表实体和关系，但他们依然对实体和关系进行分别抽取，而且制造了大量冗余信息，
* 这篇论文的思想是直接将三元组抽取出来，而不是分别抽取，提出了一种使用标注方法的端到端的模型，设计了一种包含实体和关系信息的新的标签，将联合学习转为标注问题，
* 使用由远程监督生成的公开数据集，
* 论文重要贡献，
    * 使用了新的标注策略，将联合抽取问题转化为标注问题，
    * 使用了不同的端到端的方法，论文中的方法效果最好
    * 使用了带偏差的损失函数去适应新的标签，这可以加强实体之间的联系

### Related Works
* pipline方法，经典的NER方法为线性统计模型，HMM和CRF等等，神经网络方法主要将NER任务当成连续的序列标注任务，同样的关系抽取方法也可以分为基于手工特征的方法和基于神经网络的方法，
* 联合模型提取关系和实体共用一个模型，大多数联合模型是基于特征的，用LSTM模型则可以减少人工，
* 端到端的模型是将输入句子转成向量，大多数模型方法用Bi-LSTM对输入句子进行编码，但是解码方法各不相同，比如用CRF或者LSTM等

### Method
* The Tagging Scheme
    * "O"代表"Other"
    * "BIES"代表"Begin,Inside,End,Single",由三部分組成，position in the entity, the relation type, and the relation role  (Entity1;RelationT ype;Entity2)
    * 共有2*4*R+1种标签
* From Tag Sequence To Extracted Results
    * 按照就近原则，只考虑一个实体属于一个三元组，不考虑有重叠关系的情况
* The End-to-end Model
    * Bi-LSTM+LSTM
    * Bi-LSTM Encoder：forward lstm layer, backward lstm layer and the concatenate layer.word embedding layer将1-hot向量转换为embedding，
    * 最后是softmax，输出可能的tag种类
    * The Bias Objective Function

### Experiment
* 数据集：
    * NYT，由远程监督产生，training data contains 353k triplets, and the test set contains 3; 880 triplets. Besides, the size of relation set is 24.
    * 训练集训练产生word_embedding
* Baseline
    * piplined methods：CoType，DS-logistic，LINE，FCM
    * jointly-methods:DS-Joint, MultiR, CoType, 
    * end-to-end: LSTM-CRF, LSTM-LSTM
* result
    * tagging>joint>pipline
    * LSTM-LSTM>LSTM-CRF:LSTM擅长于学习长距离依赖，而CRF擅长于学习整个序列的联合概率

### Analysis and Discussion
* 单独识别实体的查准率会比实体二元组的查准率要低，说明有很多实体并没有找到配对，而成了独立的实体，二元组的识别结果比三元组的结果好，说明有一些关系预测错误
* 由单独实体在所有实体中的比例，可以看出，论文使用的模型和其他模型相比，能使头尾实体之间的联系加强，

### Conclusion
* 新的标注策略，端到端的联合抽取的方法
* 无法对有重叠的关系进行抽取，将来会将输出层的softmax换成multiple classifier，所以一个词可以有多个标签

---
## [Joint entity and relation extraction based on a hybrid neural network](https://pdf.sciencedirectassets.com/271597/1-s2.0-S0925231217X00301/1-s2.0-S0925231217301613/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjENf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCqKhz6oXKFX%2FZuwfU6L88il3j4rp1ezK2QKp9nnqJNSgIgIPAZi%2BlQS6U8Ibx4KVhEAHTMeb5muX7t0BY8pajebccq4wMIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDD2szH6BUUNZFt39ISq3A7yDu4CbhJIvN7%2FaYIwhMCExdkUZ3UC1BoGLZnDEFs3KOeb7FVKoyfOyi4Q9BaWnD3cy%2FPhRprMgTTHdZJaBNHdOY%2Bkc%2BFGzPcSmqt1bQycDeSvgalceCGnk9ia2ES5rKZUYDcH3g%2F6y3hGR%2BxycC2PemGQ3YhSh7oO9zo5T6MkJn3ia6I9AAfzxKdkD23S37lALcCVjzTZOtwOoRHd8JuLGjX4KFhqi6rQ%2BXq1poZTyXX0dN4isrm2%2B0H9DAXrHT7R7azDgQJL7Fpg3cnrOHvejEZ4psenQsRTCdDuc%2B90zAnyw6by7HWdgYGFy9NiSO%2F%2FDAvLMQLdCGhdPmaPB%2BC2uilAA%2FXn1FQ0aXogdrQ1vXoeTOXXPXlmeqsVuDTw9HCwHxBPwk%2BHrjeQ3MgIMmiGacuPphTqVhrHWOr0m0Ktx6mLjU4lh7zZQzTVBYw0AkFNVfpSJ%2FOMjx%2BijgAFpe9SUm74EcZ28tG89Ri8u%2FzDwKB7FrITpCuPcPNI3odrHHjUd13%2Feg9VDbxoGnSFVVgH4FGKSb%2BKtOrqNgAtoV0RHAa79J8%2FjxUr9M81nPea1ZQVUcDxCEvMwxdbA5QU6tAHsNQjsUb%2BZH3TJeJzbHm7BxAOuduefPG7uSjHyCZ6MqJ%2F8SUeiKs%2FXAAKqerroKWOv3lJwpP7dsnLnsAf0JLnuqeD0nELoPW%2FkFcBgFrn24AbR%2FjixzmrXbhvdaUE51ItXKGHC74r7N3Pm4uJQnRXH6G3DzTrBgm4AmTL8WJ9UfmjYcBY9zThTuJ1psHmcvgL9ixsWplhR2q7gal2yNeFclbURUoNwhPHfhj0hoY53ywBFRrM%3D&AWSAccessKeyId=ASIAQ3PHCVTYXD3JNGV2&Expires=1555053484&Signature=mRoPLC%2FTFcvVNsORDS%2FvU8Ms9GE%3D&hash=92c0207e02558fbf5bf55346d1251e014c024b2f24e921155697d63307059935&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0925231217301613&tid=spdf-786dcef2-dba7-4b6e-8c49-1d3c3f136207&sid=4fd0a02628b08840bb5982c28f6b3b7c2a09gxrqa&type=client)

### abstract
* 提出了一种没有构造任何手工特征的混合模型，Bi-LSTM-ED+CNN

### Introduction
* NER和RC共用Bi-LSTM编码层，在解码层调整了LSTM结构捕捉tag之间的联系，用CNN进行关系分类，基于encoding和实体间的子句信息，

### RElated works
* NER
    * 几乎都用Bi-LSTM做encoder，decoder方法各不相同
* RC
    * CNN，RNN，RecNN，LSTM，FCM
* joint

### our method
* word_embedding,Bi-LSTM,CNN
* CNN:h_e1,w_e1,...,w_e2,h_e2,

### Experiment
* 窗口大小：k，卷积核个数：100
* baseline
    * pipline：CRF+ME
    * joint feature-based：joint w/Global
    * end-to-end：SPTree
    * 对于NER：双向的LSTM效果比单向的要好，BiLSTM-NN-2比 BiLSTM-NN效果要好，
* 使用子句效果比使用全句的效果要好
* 设定阈值，子句长度大于20则认为两实体没关系

---
## [End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures](https://arxiv.org/pdf/1601.00770.pdf)

### Abstract
* 同时捕获词序列和依存树的信息
* 通过共享参数联合抽取实体和关系

### Introduction
* 关于关系抽取，CNN效果会比RNN要好，

### Related Work
* 至于端到端的联合抽取，目前最多的还是基于特征的方法，

### Model
word-embedding，LSTM-RNN（序列），LSTM-RNN（依存）

* embedding-layer
    * 包括词，词性，依存类型，实体类型
* sequence-layer
    * 词和词性embedding连接作为输入
* entity detection
    * softmax
* Dependency Layer
    * SP-Tree，Sub-Tree，Full-Tree
    * 输入为sequence-layer和dependency type embedding，label embedding

### Result and Discussion
* 用Stanford neural dependency parser进行分析，词维度为200，其他为25

---
## [End-to-end neural relation extraction using deep biaffine attention](https://people.eng.unimelb.edu.au/dqnguyen/resources/ECIR2019.pdf)

---
## [Joint Extraction of Entities and Relations Based on a Novel Graph Scheme](https://www.ijcai.org/proceedings/2018/0620.pdf)

### Abstract
* 将联合任务转化为有向图

### Introduction
* 不仅可以对实体和关系之间的联系进型建模，还可以对关系之间的联系进行建模，比如A住在北京，北京位于中国，可以推出A住在中国，
* 我们通过设计一套转化规则，将实体识别和关系抽取联合任务转化为一个有向图的生成问题
* 基于统计的方法严重依赖于复杂的特征构造，而且很难考虑到一些全局特征
* 参数共享的方法依然是分开抽取实体和关系，而不是一起decode，实体和关系之间的联系没有被完全发掘，
* 标注策略间接地捕获了输出之间的联系，无法识别重叠的关系

### Problem Definition
* baseline：tagging scheme
* Graph Scheme
* 实体弧代表实体的内部结构，关系弧代表实体之间的关系，可以有多个弧
* arc-eager算法

### Method
* Search Algorithm
* token embedding + Bi-LSTM Encoding

### result
* 对长跨度的关系抽取效果比标注策略要好
    