# 2019.5.8_GPT

---
* **已完成**：
    1. 用正则匹配的方式去除记者名
    2. 基本完成事件转发链的提取
    3. 图数据库能稳定存储七天数据，七天之前的数据目前以json格式导出
    4. 完善事件聚类结果的查询，增加了一些返回特征
    5. 看了GPT和GPT-2两篇论文

* **进一步工作**
    1.  调研篇章级文本相似度计算方法
    2.  重写命名实体识别代码
    3.  完善聚类算法，评估聚类结果
    4.  命名实体识别存在空白
    5.  关键词提取效果一般
    
---
## [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [BLOG](https://openai.com/blog/better-language-models/)
* [效果逆天的通用语言模型 GPT 2.0 来了，它告诉了我们什么？—— 张俊林](https://www.infoq.cn/article/pW8YaUXjTuhC6d0p*OwX)

* Abstract
    * 语言模型不需要在特定的有监督的任务中进行训练
* Introduction
    * 语言模型无监督的进行下游的任务
* Approach
    * 语言模型在多任务学习中比分别的有监督学习要慢
    * Training Dataset：WebText
    * Input Representation：Byte Pair Encoding
    * Model：Transformer based architecture
* Experiments
    * Language Modeling
    * Children’s Book Test：类似完形填空，从十个里挑一个
    * LAMBADA：预测句子的最后一个词
    * Winograd Schema Challenge
    * Reading Comprehension：CoQA
    * Summarization
    * Translation
    * Question Answering
* 零样本学习：在问答，阅读理解，摘要和翻译都能达到不错的效果，但仍达不到专门为任务设计的模型的效果。

---
## [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* [BLOG](https://openai.com/blog/language-unsupervised/)

* Introduction
    * 对于用什么方法进行文本表示更加有效依旧不清晰
    * 无监督预训练+有监督fine-tuning
    * transformer，在机器翻译，文本生成，句法分析中表现较好
* Related Work
    * LSTM将语言模型的预测能力限制在短距离内，Transformer可以捕获长距离的语义信息
* Framework
    * Unsupervised pre-training
    * Supervised fine-tuning：加线性输出层
    * Task-specific input transformations：将输入句子对连接起来，相似度句子对没有顺序则分别把两种可能顺序作为输入
* Analysis
    * Zero-shot Behaviors：不进行有监督的fine-tune，而是利用生成模型
* Drawbacks
    * 计算量大
    * The limits and bias of learning about the world through text
    * Still brittle generalization

---

## 第6章 新型的实体识别技术
> * 基于时间模型的实体识别技术
>    * 时间衰减模型
>    * 实体突变模型
>   * 算法
>       * early binding算法



