# 2019.3.27_知识嵌入
* 看了知识嵌入的一篇综述，以及TransE，TransR等经典论文
* 试着用OpenKE训练知识嵌入，清华开源的训练好的嵌入，实体覆盖并不好，“周杰伦”，“刘翔”，“姚明”，“复旦大学”等短实体均未找到，估计是分词的问题，目测复旦开源的三元组覆盖会好一些，目前尝试把50维的向量训练出来了，更高维度的还需解决内存溢出的问题
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

----
* OpenKE 上下载的xlore知识图谱以及训练好的embedding，虽然拥有三千多万三元组，一千多万实体，十几万关系，但是在实体中尝试查找“周杰伦”，“刘翔”，“姚明”，“复旦大学”，“北京大学”等短实体均未找到，而是有大量“北京大学国际合作部”，“复旦大学附属华东医院”，“周杰伦中文网”之类的长实体，而维度只有50
* 简单处理了一下复旦知识工厂开放的三元组资源，筛选了一下，剩余三千多万三元组，九百多万实体，二十多万关系，想试着自己训练一套词向量出来，但训练出来的词向量文件太大，21.7G，
* 这些不是从现有数据中提取的三元组，而我们提取出的三元组，可能存在大量未登录词，并且可能会不断地产生未登录词，但是这些图谱中训练出的向量可能有更多的普遍的隐藏的联系，如果用自己抽取的三元组，第一无法保证准确度（和百科相比），第二可能抽取的背景知识相对较少，这两种比较类似于用现有的别人训练的词向量和自己训练词向量的区别。
* 未登录词是一个比较棘手的问题，思路之一：用在知识图谱与其相近的词来表示，
* 目前是文章级别的实体抽取（实体类型作为关系），实体之间的联系可能没有那么紧密，关系的类型也很单一，如果对文章进行完全抽取（包括关系抽取）的话，三元组的质量无法保证，需要的数据量可能很大，词嵌入的生成可能比较麻烦



----
* 对于不需要进行关系延伸计算的数据不放入图谱，这些数据可使用适应的存储并与知识图谱中实体作链接； 对于结构固定、实体属性信息丰富的实体类，使用其它数据库存储更能体现优势
* 知识图谱是基于图的数据结构，其存储方式主要有两种方式：RDF存储 和 图数据库(Graph Database)。
* 从零开始构建,灵活不受约束,基础维护的难度和工作量巨大
* 属性表：属性相似的主语聚为一张表 问题：1、 RDF灵活性 2、 属性未定查询
* 垂直分割：以谓语划分三元组表 问题：1、大量数据表 2、删除属性代价大
* Native RDF 存储方案 问题：1、6倍空间开销，如果是 (S, P, O, C) 四元组呢？ 2、更新维护代价大
* Neo4j 优点：图查询语言、图挖掘算法 缺点：分布式存储实现代价高，数据更新速度慢，大节点处理慢

* 整体原则：基础存储支撑灵活 基础存储可扩展、高可用 按需要进行数据分割 适时使用缓存和索引 善于利用现有成熟存储 保持图形部分数据的精简 不在图中作统计分析计算 在应用中进行扩充迭代

* 最佳实践：分批导入，并使用Index提升导入时查询效率按数据的类型、属性不同准备导入文件，数据主键维护，使用Constraints，尽可能保证导入数据格式正确


---

## 第2章 模型评估与选择
> * “留出法”（hold-out）直接将数据集D划分为两个互斥的集合
> * 交叉验证法，特例：留一法（Leave-One-Out）（计算复杂度高）
> * 自助法（在数据集较小，难以有效划分训练/测试集时很有用，但会改变数据集的分布）
> * 错误率与精度，查准率（P）（真正例在预测为正例中的比例，高说明预测结果中准确率高）、查全率（R）（真正例在现实正例中的比例，高说明测试用例中的正例都能被正确识别）和F1，P-R曲线（比较面积大小，平衡点，F1）
> * 推荐系统中，查准率比较重要，逃犯检索系统中，查全率比较重要，F1度量的一般形式$F_β$，macro-F1, micro-F1
> * ROC(Receiver Operating Characteristic),横轴是假正例率（FPR，假正例在真实为反例中的比例），纵轴是真正例率（TPR，查准率），AUC通过对ROC曲线下各部分的面积求和而得
> * 代价敏感错误率和代价曲线
> * 交叉验证t检验，“5×2交叉验证法”
> * 泛化误差可以分解为偏差、方差与噪声之和，噪声表达了在当前任务中任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度，**泛化性能**是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。


---
##《实体识别技术》

##第1章 概述

> * 在数据库领域，已有的方法通常是使用基于规则的方法。
> * 决定数据对象之间是否匹配的方法有，基于阈值的决定方法，及基于分类的决定方法和基于聚类的决定方法
> * 如何利用时序信息和属性演化信息来帮助实体识别是一个有意义的研究问题

---
## Knowledge Graph Embedding: A Survey of Approaches and Applications

* 两种方法: translational distance models, semantic matching models
### Translational Distance Models<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2019/a08a42d9ba9f932ea498e6e85df7a9b.png)<br>

* $f_r(h, t) = -||h + r -t||_{1/2}$
* 在一对多，多对一或多对多的情况下会产生不同实体向量趋于一致的情况
* TransH
    * r在某个超平面上，$w_r$是这个超平面的法向量，$h_⊥=h-w_r^Thw_r$代表h在超平面上的投影向量，
    * $h_⊥+r≈t_⊥$
* TransR
    * 实体和关系在不同的向量空间内
    * $h_⊥=M_rh$  (Mr是从实体空间到关系空间的投影矩阵）
* TransD
    * 映射矩阵 $M_r^1 = w_rw_h^T + I$ （why）
    * $h_⊥= M_r^1h$
* TranSparse
    * $h_⊥= M_r(θ_r)h$
    * $θ_r$表示映射矩阵稀疏度
    * 头实体和尾实体可使用相同的或不同的稀疏映射矩阵
![](https://github.com/qiuxingfa/picture_/blob/master/2019/dba2cd9cd917af448aff817575c4ac1.png)<br>

#### Gaussian Embeddings
* KG2E
* Unstructured model (UM)

### Semantic Matching Models
* RESCAL
* TATEC
* HolE
* ANALOGY

### Matching with Neural Networks
![](https://github.com/qiuxingfa/picture_/blob/master/2019/c7f84d4df4fc32d411ff4c40ac6dc6e.png)<br>

* Semantic Matching Energy (SME)
* Neural Tensor Network (NTN)<br>

![](https://github.com/qiuxingfa/picture_/blob/master/2019/bab2dc7af64c7b85cec91f2c70cb7e9.png)<br>

* 将实体和关系都看成向量的模型(e.g., TransE, TransH, DistMult, and ComplEx)更有效率
* 将关系看成矩阵或者张量的模型(e.g., TransR, SE, and RESCAL)在时间和空间上具有更高的复杂度
* 神经网络则拥有更高的复杂度，NTN效果比MLP效果要略差一些，甚至比TransE和DistMult这些简单模型的效果要差，HolE效果较好，

### 其他信息
* 除三元组之外，还有很多其他信息可以利用，如实体类型，关系路径，文本摘要，以及逻辑关系

---
## [OpenKE: An Open Toolkit for Knowledge Embedding](http://aclweb.org/anthology/D18-2024)
* 有训练好的embedding可供使用
* propose an offset-based negative sampling algorithm to generate negative triples
* 评估 FB15K and WN18

---
## [Knowledge Graph Embedding by Translating on Hyperplanes](https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/viewFile/8531/8546)

* TransE 在处理一对多、多对一或者多对多的关系时存在问题
* translation on hyperplanes (TransH)
