# 2018.10.10 & 10.17

---

* 使用论文 **Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components** 中的方法对word2vec产生的词向量进行评估
* 原文使用的是Chinese Wikipedia Dump进行训练，我使用的是自己在网上爬取的 @人民网，@人民日报 等十个新闻媒体账号近一百万条微博进行训练，使用和原文一样的训练参数<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/22eae216b2aeb09c8e0afb737ebd815.png)<br>
* Word Similarity <br>
    使用gensim中的evaluate_word_pairs函数，通过计算两个词的Cosine similarity，与人工标注的相似度进行比较，计算两组数据的spearman相似度,关注于两组数据的相关性而不是具体值的大小<br>
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/b69578f3203ecf1b85b1a0929772b376ae07a3ce)

    We select two different Chinese word similarity datasets, wordsim-240 and wordsim-296 provided by (Chen et al., 2015) for evaluation<br>
    wordsim-240 中有1个未登录词<br>
    wordsim-296 中有22个未登录词<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/8c0507aa8b094ba5227371fe1bf9e89.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/ca7491ef0770b7d9ad7ee3a910e313b.png)<br>

    |Model  |Wordsim-240    |Wordsim-295    |
    | ----   | -----  | ---- |
    |CBOW    |0.5272  |0.5786  |
    |skip-gram   |0.5886  |0.5844  |

    CWE : Chinese word embeddings<br>
    MGE : multi-granularity embedding<br>
    JWE: joint learning word embedding<br>
    +c represents the components feature<br>     +r represents the radicals feature<br> 
    +p indicates which subcharacters are used to predict the target word<br>
* Word Analogy<br>
    利用gensim中的evaluate_word_analogies函数，文本格式如“雅典 希腊 巴格达 伊拉克”，计算与向量“希腊”-“雅典”+“巴格达”最接近的词，与文本中的词进行比较
测试文本里共有3种词对Capital，State，Family<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/b2cc582d58c7359a015bfc761490c3f.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/6a63291274d2989b6ea1c0be625cc26.png)<br>

    |Model  |Total  |Capital    |State  |Family |
    | ----   | -----  | ---- |  ----   | -----  |
    |CBOW    |0.7811  |0.7947  |0.9657  |0.6287  |
    |skip-gram   |0.8559  |0.9350 |1.0  |0.5662 |
    
    
    总体上skip-gram效果比CBOW要好一些，其中‘state’一栏效果很好，skip-gram模型正确率甚至达到了100%，可能原因是语料中有很大一部分国内新闻，经常会提及国内地名

* 思考：
    * 测试文件的数据是否偏少，是否具有代表性，测试方法是否具有可信度？
    * 一个向量能不能代表一个词，有没有必要使用多向量表示？
    * 各种字特征，位置特征等等对于词向量的作用有多大？



--------------------
## How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks

1. Evaluation methods of word embeddings can be roughly divided into two groups: `extrinsic`(used in a down stream task eg. POS tagging) and `intrinsic`(Word Similarity).Both intrinsic and downstream evaluations are criticized.
2. First argument for data efficiency focused evaluation is the growing evidence that pretrained word embeddings provide little benefit under various settings, especially deep learning models
3. It has been observed in practice that word embeddings have useful information `only in a small subspace`

----------------------
## 下周计划
* 尝试不同的训练模型
* 测试在具体任务中使用预训练好的词向量
* 看论文
        
